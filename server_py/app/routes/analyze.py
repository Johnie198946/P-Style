"""
åˆ†æè·¯ç”± - Part1/Part2/Feasibility/AI è¯Šæ–­
æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 4ã€16ã€26 èŠ‚å®ç°ï¼Œæ–°å¢ AI è¯Šæ–­æ¥å£
æä¾›å¯è¡Œæ€§è¯„ä¼°ã€ä¸¤é˜¶æ®µåˆ†æã€AI è¯Šæ–­å’Œä»»åŠ¡æŸ¥è¯¢æ¥å£
"""
import json
import asyncio  # ç”¨äºå¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ
import time  # ç”¨äºè®°å½•è¯·æ±‚å¤„ç†æ—¶é—´
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError  # ç”¨äºæ˜¾è‘—æ€§æ£€æµ‹çš„è¶…æ—¶æ§åˆ¶
from fastapi import APIRouter, Depends, HTTPException, Form, Request, Body
from fastapi.security import HTTPAuthorizationCredentials
from sqlalchemy.orm import Session
from sqlalchemy import func, desc  # func ç”¨äº count ç»Ÿè®¡ï¼Œdesc ç”¨äºæ’åº
from typing import Optional, Dict, Any, List
from loguru import logger  # æ—¥å¿—è®°å½•å·¥å…·
from pydantic import BaseModel

from ..db import get_db
from ..models import User, AnalysisTask, Upload
from ..middleware.auth import get_current_user, security
from ..services.feasibility_service import FeasibilityService
from ..services.gemini_service import get_gemini_service
from ..services.prompt_template import PromptTemplateService
from ..services.analysis_formatter import AnalysisFormatter
from ..services.task_service import TaskService
from ..services.usage_service import UsageService
from ..services.saliency_service import SaliencyService  # ã€æ–°å¢ã€‘æ˜¾è‘—æ€§æ£€æµ‹æœåŠ¡
from ..utils.response import success_response, error_response
from ..constants.error_codes import ErrorCode
from ..schemas.analysis_schemas import (
    Part1RequestSchema,
    Part2RequestSchema,
    DiagnosisRequestSchema,
    validate_diagnosis_response
)

router = APIRouter(prefix="/api/analyze", tags=["analyze"])

# åˆå§‹åŒ–æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
feasibility_service = FeasibilityService()  # å¯è¡Œæ€§è¯„ä¼°æœåŠ¡ï¼ˆCV ç®—æ³•ï¼‰
gemini_service = get_gemini_service()  # Gemini API æœåŠ¡
prompt_template = PromptTemplateService()  # Prompt æ¨¡æ¿æœåŠ¡
formatter = AnalysisFormatter()  # ç»“æœæ ¼å¼åŒ–æœåŠ¡
task_service = TaskService()  # ä»»åŠ¡ç®¡ç†æœåŠ¡
usage_service = UsageService()  # ç”¨é‡ç»Ÿè®¡æœåŠ¡
saliency_service = SaliencyService()  # ã€æ–°å¢ã€‘æ˜¾è‘—æ€§æ£€æµ‹æœåŠ¡ï¼ˆç”¨äºç”Ÿæˆè§†è§‰é‡å¿ƒé®ç½©å›¾ï¼‰


@router.post("/feasibility")
async def analyze_feasibility(
    request: Request,  # ã€é‡è¦ã€‘æ·»åŠ  Request å‚æ•°ï¼Œç”¨äºåœ¨ Form è§£æä¹‹å‰è®°å½•æ—¥å¿—
    sourceImage: str = Form(...),
    targetImage: str = Form(...),
    taskId: Optional[str] = Form(None),
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """
    å¤åˆ»å¯è¡Œæ€§è¯„ä¼°æ¥å£
    æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 26 èŠ‚å®ç°ï¼Œç”±ç³»ç»Ÿ CV ç®—æ³•ä¸»å¯¼ï¼Œä¸ä¾èµ– Gemini
    
    Args:
        sourceImage: å‚è€ƒå›¾ï¼ˆbase64 æˆ– data URLï¼Œå¿…å¡«ï¼‰
        targetImage: ç”¨æˆ·å›¾ï¼ˆbase64 æˆ– data URLï¼Œå¿…å¡«ï¼‰
        taskId: å¯é€‰çš„ä»»åŠ¡ IDï¼Œç”¨äºå…³è”å¯è¡Œæ€§ç»“æœ
        credentials: JWT Tokenï¼ˆBearerï¼Œå¿…å¡«ï¼‰
        db: æ•°æ®åº“ä¼šè¯
    
    Returns:
        {
            "code": 0,
            "message": "ok",
            "data": {
                "feasibilityScore": 0.614,
                "difficulty": "ä¸­",
                "confidence": 0.78,
                "dealBreakers": [],
                "dominantFactors": [...],
                "recommendedActions": [...],
                "metrics": {...},
                "explanation": "..."
            }
        }
    
    Raises:
        HTTPException: å¦‚æœå‚æ•°éªŒè¯å¤±è´¥ã€ç”¨æˆ·æœªç™»å½•ã€æˆ–è¯„ä¼°è¿‡ç¨‹å‡ºé”™
    
    Note:
        - å¯è¡Œæ€§è¯„ä¼°ä¸è®¡å…¥å•ç‹¬ç”¨é‡ï¼Œå› ä¸ºå®ƒé€šå¸¸ä½œä¸º Part1 çš„å‰ç½®æ­¥éª¤
        - Part1 æ¥å£ä¼šæ£€æŸ¥ç”¨é‡é™åˆ¶ï¼Œå› æ­¤è¿™é‡Œä¸éœ€è¦é‡å¤æ£€æŸ¥
        - å›¾ç‰‡æ•°æ®å¯ä»¥æ˜¯ base64 å­—ç¬¦ä¸²æˆ– data URL æ ¼å¼ï¼ˆå¦‚ data:image/jpeg;base64,...ï¼‰
    """
    try:
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•å‡½æ•°å…¥å£ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜
        # æ³¨æ„ï¼šåœ¨ FastAPI çš„ Form(...) å‚æ•°è§£æä¹‹å‰ï¼Œå¦‚æœè¯·æ±‚æ ¼å¼ä¸æ­£ç¡®ï¼Œä¼šæŠ›å‡º RequestValidationError
        # è¿™ä¸ªå¼‚å¸¸ä¼šè¢« main.py ä¸­çš„ request_validation_exception_handler æ•è·
        # å¦‚æœå‡½æ•°è¢«è°ƒç”¨ï¼Œè¯´æ˜ Form æ•°æ®è§£ææˆåŠŸï¼Œå‚æ•°å·²ç»æ­£ç¡®ä¼ é€’
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å‡½æ•°è¢«è°ƒç”¨ï¼Œå¼€å§‹å¤„ç†è¯·æ±‚")
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘è¯·æ±‚è·¯å¾„: {request.url.path}")
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘è¯·æ±‚æ–¹æ³•: {request.method}")
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘Content-Type: {request.headers.get('content-type', 'æœªçŸ¥')}")
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘Content-Length: {request.headers.get('content-length', 'æœªçŸ¥')}")
        logger.debug(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘sourceImage ç±»å‹: {type(sourceImage)}, æ˜¯å¦ä¸º None: {sourceImage is None}")
        logger.debug(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘targetImage ç±»å‹: {type(targetImage)}, æ˜¯å¦ä¸º None: {targetImage is None}")
        
        # ã€å‚æ•°éªŒè¯ã€‘ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥å‚æ•°æ˜¯å¦å­˜åœ¨ï¼ˆNone æ£€æŸ¥ï¼‰
        # æ³¨æ„ï¼šFastAPI çš„ Form(...) å‚æ•°åœ¨è§£æå¤±è´¥æ—¶ä¼šæŠ›å‡º RequestValidationError
        # ä½†å¦‚æœè¯·æ±‚æ ¼å¼æ­£ç¡®ä½†å­—æ®µå€¼ä¸ºç©ºï¼ŒForm(...) å¯èƒ½è¿”å› None æˆ–ç©ºå­—ç¬¦ä¸²
        # è¿™é‡Œéœ€è¦å…ˆæ£€æŸ¥å‚æ•°æ˜¯å¦å­˜åœ¨ï¼ˆå¯èƒ½ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼‰
        if sourceImage is None:
            logger.error("ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: sourceImage å‚æ•°ä¸º None")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "å‚è€ƒå›¾ï¼ˆsourceImageï¼‰ä¸èƒ½ä¸ºç©º")
        
        # ã€å‚æ•°éªŒè¯ã€‘ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥å‚æ•°ç±»å‹ï¼ˆç±»å‹æ£€æŸ¥ï¼‰
        # æ³¨æ„ï¼šFastAPI çš„ Form(...) åº”è¯¥è¿”å›å­—ç¬¦ä¸²ï¼Œä½†å¦‚æœå‰ç«¯å‘é€çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œå¯èƒ½è¿”å›å…¶ä»–ç±»å‹
        if not isinstance(sourceImage, str):
            logger.error(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: sourceImage ç±»å‹ä¸æ­£ç¡®ï¼ŒæœŸæœ› strï¼Œå®é™… {type(sourceImage).__name__}")
            raise error_response(ErrorCode.INVALID_REQUEST, f"å‚è€ƒå›¾ï¼ˆsourceImageï¼‰æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—ç¬¦ä¸²ï¼Œå®é™…ç±»å‹: {type(sourceImage).__name__}")
        
        # ã€å‚æ•°éªŒè¯ã€‘ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥å‚æ•°å€¼æ˜¯å¦ä¸ºç©ºï¼ˆç©ºå€¼æ£€æŸ¥ï¼‰
        # æ³¨æ„ï¼šå³ä½¿å‚æ•°ä¸æ˜¯ None ä¸”ç±»å‹æ­£ç¡®ï¼Œä¹Ÿå¯èƒ½åªåŒ…å«ç©ºç™½å­—ç¬¦
        if not sourceImage.strip():
            logger.error("ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: sourceImage å‚æ•°ä¸ºç©ºå­—ç¬¦ä¸²")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "å‚è€ƒå›¾ï¼ˆsourceImageï¼‰ä¸èƒ½ä¸ºç©º")
        
        # ã€å‚æ•°éªŒè¯ã€‘å¯¹ targetImage æ‰§è¡Œç›¸åŒçš„ä¸‰æ­¥éªŒè¯
        if targetImage is None:
            logger.error("ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: targetImage å‚æ•°ä¸º None")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "ç”¨æˆ·å›¾ï¼ˆtargetImageï¼‰ä¸èƒ½ä¸ºç©º")
        
        if not isinstance(targetImage, str):
            logger.error(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: targetImage ç±»å‹ä¸æ­£ç¡®ï¼ŒæœŸæœ› strï¼Œå®é™… {type(targetImage).__name__}")
            raise error_response(ErrorCode.INVALID_REQUEST, f"ç”¨æˆ·å›¾ï¼ˆtargetImageï¼‰æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—ç¬¦ä¸²ï¼Œå®é™…ç±»å‹: {type(targetImage).__name__}")
        
        if not targetImage.strip():
            logger.error("ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: targetImage å‚æ•°ä¸ºç©ºå­—ç¬¦ä¸²")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "ç”¨æˆ·å›¾ï¼ˆtargetImageï¼‰ä¸èƒ½ä¸ºç©º")
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•è¯·æ±‚ä¿¡æ¯ï¼ˆä¸è®°å½•å®Œæ•´çš„å›¾ç‰‡æ•°æ®ï¼Œåªè®°å½•æ•°æ®é•¿åº¦å’Œæ ¼å¼ï¼‰
        # æ³¨æ„ï¼šå›¾ç‰‡æ•°æ®å¯èƒ½å¾ˆå¤§ï¼ˆå‡ MBåˆ°å‡ åMBï¼‰ï¼Œå®Œæ•´è®°å½•ä¼šå¯¼è‡´æ—¥å¿—æ–‡ä»¶è¿‡å¤§
        # åªè®°å½•æ•°æ®é•¿åº¦å’Œæ ¼å¼ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜ï¼ŒåŒæ—¶é¿å…æ—¥å¿—æ–‡ä»¶è¿‡å¤§
        source_image_length = len(sourceImage)
        target_image_length = len(targetImage)
        source_image_format = "data URL" if sourceImage.startswith("data:image") else "base64"
        target_image_format = "data URL" if targetImage.startswith("data:image") else "base64"
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘è¯·æ±‚ä¿¡æ¯: sourceImageé•¿åº¦={source_image_length}, æ ¼å¼={source_image_format}, targetImageé•¿åº¦={target_image_length}, æ ¼å¼={target_image_format}, taskId={taskId}")
        
        # ã€èº«ä»½éªŒè¯ã€‘éªŒè¯ç”¨æˆ·èº«ä»½ï¼ˆæ ¹æ®æ³¨å†Œç™»å½•ä¸æƒé™è®¾è®¡æ–¹æ¡ˆï¼Œæ‰€æœ‰åˆ†ææ¥å£éœ€è¦ç™»å½•ï¼‰
        # æ³¨æ„ï¼šsecurity ä¾èµ–ï¼ˆHTTPBearerï¼‰ä¼šåœ¨ Token æ— æ•ˆæ—¶æŠ›å‡º HTTPException (403)
        # å¦‚æœ Token æœ‰æ•ˆï¼Œget_current_user ä¼šè¿”å›å½“å‰ç”¨æˆ·å¯¹è±¡
        current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)
        logger.debug(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘ç”¨æˆ·èº«ä»½éªŒè¯æˆåŠŸ: ç”¨æˆ· {current_user.email} (ID: {current_user.id})")
        
        # ã€ä¸šåŠ¡é€»è¾‘ã€‘è°ƒç”¨ CV ç®—æ³•è¿›è¡Œå¯è¡Œæ€§è¯„ä¼°ï¼ˆç³»ç»Ÿç®—æ³•ä¸»å¯¼ï¼Œä¸ä¾èµ– Geminiï¼‰
        # æ³¨æ„ï¼š
        # 1. feasibility_service.evaluate æ–¹æ³•å†…éƒ¨å·²ç»å¤„ç†äº†å›¾ç‰‡è§£ç å’Œå¼‚å¸¸
        # 2. å¦‚æœå›¾ç‰‡è§£ç å¤±è´¥ï¼Œä¼šè¿”å›é”™è¯¯ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        # 3. å›¾ç‰‡æ•°æ®å¯ä»¥æ˜¯ data URL æ ¼å¼ï¼ˆdata:image/jpeg;base64,...ï¼‰æˆ–çº¯ base64 å­—ç¬¦ä¸²
        # 4. feasibility_service ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶è§£æä¸¤ç§æ ¼å¼
        result = feasibility_service.evaluate(sourceImage, targetImage)
        
        # ã€ç»“æœéªŒè¯ã€‘æ£€æŸ¥è¯„ä¼°ç»“æœæ˜¯å¦åŒ…å«é”™è¯¯
        # æ³¨æ„ï¼šå¦‚æœå›¾ç‰‡è§£ç å¤±è´¥ï¼Œfeasibility_service ä¼šåœ¨ dealBreakers ä¸­æ·»åŠ é”™è¯¯ä¿¡æ¯
        # è¿™é‡Œéœ€è¦æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡å¤„ç†é”™è¯¯ï¼Œå¦‚æœæ˜¯ï¼Œåº”è¯¥æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›é”™è¯¯ç»“æœ
        if result.get("dealBreakers") and len(result.get("dealBreakers", [])) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡è§£ç é”™è¯¯
            deal_breakers = result.get("dealBreakers", [])
            if any("æ— æ³•è§£ç " in str(breaker) or "è¯„ä¼°å¤±è´¥" in str(breaker) for breaker in deal_breakers):
                logger.error(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘å¤±è´¥: å›¾ç‰‡è§£ç æˆ–å¤„ç†é”™è¯¯, dealBreakers={deal_breakers}")
                raise error_response(ErrorCode.IMAGE_PROCESSING_FAILED, f"å›¾ç‰‡å¤„ç†å¤±è´¥: {', '.join(deal_breakers)}")
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•è¯„ä¼°æˆåŠŸä¿¡æ¯
        logger.info(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘æˆåŠŸ: feasibilityScore={result.get('feasibilityScore')}, difficulty={result.get('difficulty')}, user_id={current_user.id}")
        
        # ã€æ•°æ®æŒä¹…åŒ–ã€‘å¦‚æœæä¾›äº† taskIdï¼Œå°†ç»“æœä¿å­˜åˆ°ä»»åŠ¡è®°å½•
        # æ³¨æ„ï¼štaskId æ˜¯å¯é€‰çš„ï¼Œå¦‚æœå‰ç«¯æ²¡æœ‰æä¾›ï¼Œå°±ä¸ä¿å­˜åˆ°ä»»åŠ¡è®°å½•
        # å¯è¡Œæ€§è¯„ä¼°ç»“æœå¯ä»¥ç‹¬ç«‹å­˜åœ¨ï¼Œä¸ä¸€å®šéœ€è¦å…³è”åˆ°ä»»åŠ¡
        if taskId:
            task = task_service.get_task(db, taskId)
            if task:
                task.feasibility_result = result
                db.commit()
                logger.debug(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘ç»“æœå·²ä¿å­˜åˆ°ä»»åŠ¡: taskId={taskId}")
            else:
                logger.warning(f"ã€å¯è¡Œæ€§è¯„ä¼°ã€‘ä»»åŠ¡ä¸å­˜åœ¨: taskId={taskId}ï¼Œè·³è¿‡ç»“æœä¿å­˜")

        # ã€å“åº”è¿”å›ã€‘è¿”å›ç»Ÿä¸€æ ¼å¼çš„æˆåŠŸå“åº”
        # æ³¨æ„ï¼šæ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 15 èŠ‚ï¼Œæ‰€æœ‰æ¥å£å¿…é¡»è¿”å› {code, message, data} æ ¼å¼
        return success_response(data=result)
    except HTTPException:
        # é‡æ–°æŠ›å‡º HTTPExceptionï¼ˆå¦‚ error_response è¿”å›çš„å¼‚å¸¸ï¼‰
        raise
    except Exception as e:
        # æ•è·å…¶ä»–æœªé¢„æœŸçš„å¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        error_type = type(e).__name__
        error_detail = str(e)
        logger.exception(f"å¯è¡Œæ€§è¯„ä¼°æ—¶å‘ç”Ÿæœªé¢„æœŸçš„å¼‚å¸¸: {error_type}: {error_detail}")
        raise error_response(ErrorCode.FEASIBILITY_CHECK_FAILED, f"å¯è¡Œæ€§è¯„ä¼°å¤±è´¥: {error_detail}")


@router.post("/part1")
async def analyze_part1(
    request_data: Part1RequestSchema = Body(...),
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """
    Part1 åˆ†ææ¥å£
    æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 23.2 èŠ‚å’Œç¬¬ 763 è¡Œå®ç°ï¼Œè¾“å‡ºåŸºç¡€æ´å¯Ÿï¼ˆç‚¹è¯„ã€æ„å›¾ã€å…‰å½±è¶‹åŠ¿ã€å¯è¡Œæ€§è¯´æ˜ã€å·¥ä½œæµè‰æ¡ˆï¼‰
    
    Args:
        request_data: è¯·æ±‚æ•°æ®
            {
                "uploadId": str,  # ä¸Šä¼ è®°å½• IDï¼ˆå¿…å¡«ï¼Œæ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 763 è¡Œï¼‰
                "optional_style": str  # å¯é€‰é£æ ¼å…³é”®è¯ï¼ˆå¦‚ "æ—¥å‡ºæš–å…‰", "èƒ¶ç‰‡æ„Ÿ"ï¼‰
            }
        credentials: JWT Tokenï¼ˆBearerï¼‰
        db: æ•°æ®åº“ä¼šè¯
    
    Returns:
        {
            "code": 0,
            "message": "ok",
            "data": {
                "taskId": "uuid",
                "stage": "part1",
                "status": "part1_completed",
                "structuredAnalysis": {...},  # æ ‡å‡†åŒ–çš„ Part1 ç»“æ„
                "naturalLanguage": "...",  # è‡ªç„¶è¯­è¨€æŠ¥å‘Š
                "protocolVersion": "2025-02"
            }
        }
        
    Note:
        - éœ€è¦å…ˆæ£€æŸ¥ç”¨æˆ·ç”¨é‡é™åˆ¶ï¼ˆPart1+Part2 è®¡ 1 æ¬¡åˆ†æï¼‰
        - æ ¹æ® uploadId ä»æ•°æ®åº“è·å–å›¾ç‰‡æ•°æ®ï¼ˆsource_image_data å’Œ target_image_dataï¼‰
        - å¦‚æœæä¾›äº† targetImageï¼Œä¼šå…ˆè¿›è¡Œå¯è¡Œæ€§è¯„ä¼°
        - è°ƒç”¨ Gemini API ç”Ÿæˆåˆ†æç»“æœï¼Œç„¶åæ ¼å¼åŒ–å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    """
    try:
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•å‡½æ•°å…¥å£ï¼Œä¾¿äºè¿½è¸ªé—®é¢˜  
        # ã€å¢å¼ºæ—¥å¿—ã€‘è®°å½•è¯·æ±‚æ—¶é—´æˆ³ã€è¯·æ±‚å¤´ä¿¡æ¯ã€å®¢æˆ·ç«¯IPç­‰è¯¦ç»†ä¿¡æ¯
        import time
        request_start_time = time.time()
        logger.info("=" * 80)
        logger.info(f"ã€Part1 åˆ†æã€‘æ”¶åˆ°åˆ†æè¯·æ±‚")
        logger.info(f"ã€Part1 åˆ†æã€‘è¯·æ±‚æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ã€Part1 åˆ†æã€‘uploadId: {request_data.uploadId}")
        logger.info(f"ã€Part1 åˆ†æã€‘optional_style: {request_data.optional_style}")
        logger.info("=" * 80)
        
        # éªŒè¯ç”¨æˆ·èº«ä»½
        current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)
        logger.info(f"ã€Part1 åˆ†æã€‘ç”¨æˆ·è®¤è¯æˆåŠŸï¼Œç”¨æˆ·ID: {current_user.id}, ç”¨æˆ·é‚®ç®±: {current_user.email}")
        
        # ã€é‡è¦ã€‘æ ¹æ® uploadId ä»æ•°æ®åº“è·å–å›¾ç‰‡æ•°æ®
        # æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 763 è¡Œï¼Œå‰ç«¯ä¼ é€’ uploadIdï¼Œåç«¯éœ€è¦ä» Upload è¡¨ä¸­æŸ¥è¯¢å›¾ç‰‡æ•°æ®
        upload = db.query(Upload).filter(Upload.id == request_data.uploadId).first()
        if not upload:
            logger.error(f"ã€Part1 åˆ†æã€‘ä¸Šä¼ è®°å½•ä¸å­˜åœ¨: uploadId={request_data.uploadId}")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, f"ä¸Šä¼ è®°å½•ä¸å­˜åœ¨: {request_data.uploadId}")
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•æŸ¥è¯¢åˆ°çš„ä¸Šä¼ è®°å½•ä¿¡æ¯
        logger.info(f"ã€Part1 åˆ†æã€‘æŸ¥è¯¢åˆ°ä¸Šä¼ è®°å½•: uploadId={upload.id}, æœ‰æºå›¾: {upload.source_image_data is not None}, æœ‰ç›®æ ‡å›¾: {upload.target_image_data is not None}")
        
        # ã€é‡è¦ã€‘ä»ä¸Šä¼ è®°å½•ä¸­è·å–å›¾ç‰‡æ•°æ®
        # æ ¹æ®æ°¸ä¹…åŒ–å­˜å‚¨æ–¹æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨ source_image_dataï¼ˆBase64ï¼‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ä» source_image_url è·å–
        # è¿™æ ·å¯ä»¥å…¼å®¹å¯¹è±¡å­˜å‚¨æ¨¡å¼å’Œ Base64 æ¨¡å¼
        sourceImage = upload.source_image_data
        targetImage = upload.target_image_data
        
        # ã€å…¼å®¹æ€§å¤„ç†ã€‘å¦‚æœ source_image_data ä¸ºç©ºï¼Œå°è¯•ä» source_image_url è·å–
        # æ³¨æ„ï¼šå¦‚æœ source_image_url æ˜¯ data URLï¼ˆdata:image/jpeg;base64,...ï¼‰ï¼Œéœ€è¦æå– Base64 éƒ¨åˆ†
        if not sourceImage and upload.source_image_url:
            logger.info(f"ã€Part1 åˆ†æã€‘source_image_data ä¸ºç©ºï¼Œå°è¯•ä» source_image_url è·å–: {upload.source_image_url[:100]}...")
            if upload.source_image_url.startswith("data:"):
                # æå– Base64 éƒ¨åˆ†ï¼ˆæ ¼å¼ï¼šdata:image/jpeg;base64,<base64_data>ï¼‰
                try:
                    # æ‰¾åˆ° base64, åé¢çš„éƒ¨åˆ†
                    base64_part = upload.source_image_url.split("base64,")[1] if "base64," in upload.source_image_url else None
                    if base64_part:
                        sourceImage = base64_part
                        logger.info(f"ã€Part1 åˆ†æã€‘ä» source_image_url æå– Base64 æ•°æ®æˆåŠŸï¼Œé•¿åº¦: {len(sourceImage)} å­—ç¬¦")
                    else:
                        logger.warning(f"ã€Part1 åˆ†æã€‘source_image_url æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•æå– Base64 æ•°æ®")
                except Exception as e:
                    logger.error(f"ã€Part1 åˆ†æã€‘ä» source_image_url æå– Base64 æ•°æ®å¤±è´¥: {type(e).__name__}: {str(e)}")
            else:
                # å¦‚æœæ˜¯å¯¹è±¡å­˜å‚¨ URLï¼Œéœ€è¦ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º Base64
                # æ³¨æ„ï¼šè¿™éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œå¯èƒ½ä¼šå¾ˆæ…¢ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨ source_image_data
                logger.warning(f"ã€Part1 åˆ†æã€‘source_image_url æ˜¯å¯¹è±¡å­˜å‚¨ URLï¼Œéœ€è¦ä¸‹è½½å›¾ç‰‡ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ…¢")
                # TODO: å®ç°ä»å¯¹è±¡å­˜å‚¨ URL ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º Base64 çš„é€»è¾‘
                # å½“å‰æš‚æ—¶ä¸æ”¯æŒï¼Œéœ€è¦å‰ç«¯ç¡®ä¿ä¸Šä¼ æ—¶ä¿å­˜ Base64 æ•°æ®
        
        # ã€éªŒè¯ã€‘ç¡®ä¿æºå›¾æ•°æ®å­˜åœ¨
        if not sourceImage:
            logger.error(f"ã€Part1 åˆ†æã€‘æºå›¾æ•°æ®ä¸ºç©º: uploadId={request_data.uploadId}")
            logger.error(f"ã€Part1 åˆ†æã€‘ä¸Šä¼ è®°å½•è¯¦æƒ…: source_image_data={upload.source_image_data is not None}, source_image_url={upload.source_image_url is not None}, target_image_data={upload.target_image_data is not None}, target_image_url={upload.target_image_url is not None}")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "æºå›¾æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•å›¾ç‰‡æ•°æ®ä¿¡æ¯ï¼ˆä¸è®°å½•å®Œæ•´æ•°æ®ï¼Œåªè®°å½•é•¿åº¦ï¼‰
        source_image_size = len(sourceImage) if sourceImage else 0
        target_image_size = len(targetImage) if targetImage else 0
        logger.info(f"ã€Part1 åˆ†æã€‘æºå›¾æ•°æ®é•¿åº¦: {source_image_size} å­—ç¬¦, ç›®æ ‡å›¾æ•°æ®é•¿åº¦: {target_image_size} å­—ç¬¦")
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ£€æŸ¥å›¾ç‰‡æ•°æ®å¤§å°ï¼Œå¦‚æœè¿‡å¤§åˆ™è­¦å‘Š
        # Base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®é€šå¸¸æ¯”åŸå§‹å›¾ç‰‡å¤§çº¦ 33%ï¼Œ10MB çš„å›¾ç‰‡çº¦ç­‰äº 13MB çš„ Base64 å­—ç¬¦ä¸²
        MAX_IMAGE_SIZE = 20 * 1024 * 1024  # 20MB Base64 å­—ç¬¦ä¸²ï¼ˆçº¦ 15MB åŸå§‹å›¾ç‰‡ï¼‰
        if source_image_size > MAX_IMAGE_SIZE:
            logger.warning(f"ã€Part1 åˆ†æã€‘âš ï¸ æºå›¾æ•°æ®è¿‡å¤§: {source_image_size / 1024 / 1024:.2f}MBï¼Œå¯èƒ½å½±å“å¤„ç†é€Ÿåº¦")
        if target_image_size > MAX_IMAGE_SIZE:
            logger.warning(f"ã€Part1 åˆ†æã€‘âš ï¸ ç›®æ ‡å›¾æ•°æ®è¿‡å¤§: {target_image_size / 1024 / 1024:.2f}MBï¼Œå¯èƒ½å½±å“å¤„ç†é€Ÿåº¦")
        
        # æ£€æŸ¥ç”¨é‡é™åˆ¶ï¼ˆä¸¥æ ¼é™æµï¼Œè¶…å‡ºåˆ™è¿”å›é”™è¯¯ç ï¼‰
        # æ³¨æ„ï¼šç®¡ç†å‘˜è´¦å·ä¸å—ç”¨é‡é™åˆ¶ï¼ˆæ ¹æ®å¼€å‘æ–¹æ¡ˆï¼Œç®¡ç†å‘˜æ‹¥æœ‰æ‰€æœ‰æƒé™ï¼‰
        allowed, error_code = usage_service.check_usage_limit(db, current_user.id, "analysis", user_role=current_user.role)
        if not allowed:
            logger.warning(f"ã€Part1 åˆ†æã€‘ç”¨æˆ·ç”¨é‡å·²è¾¾ä¸Šé™: user_id={current_user.id}")
            raise error_response(error_code, "åˆ†ææ¬¡æ•°å·²è¾¾ä¸Šé™")

        # å¦‚æœæä¾›äº†ç›®æ ‡å›¾ï¼Œå…ˆè¿›è¡Œå¯è¡Œæ€§è¯„ä¼°
        feasibility_result = None
        if targetImage:
            logger.info(f"ã€Part1 åˆ†æã€‘å¼€å§‹å¯è¡Œæ€§è¯„ä¼°...")
            feasibility_result = feasibility_service.evaluate(sourceImage, targetImage)
            logger.info(f"ã€Part1 åˆ†æã€‘å¯è¡Œæ€§è¯„ä¼°å®Œæˆ: feasibilityScore={feasibility_result.get('feasibilityScore') if feasibility_result else 'None'}")

        # åˆ›å»ºåˆ†æä»»åŠ¡è®°å½•ï¼ˆæ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿æ•°æ®åº“æ“ä½œå¤±è´¥æ—¶èƒ½æ­£ç¡®è¿”å›é”™è¯¯ï¼‰
        try:
            task = task_service.create_task(db, current_user.id, sourceImage, targetImage)
            logger.info(f"ã€Part1 åˆ†æã€‘ä»»åŠ¡åˆ›å»ºæˆåŠŸ: taskId={task.id}")
        except Exception as db_error:
            # æ•°æ®åº“æ“ä½œå¤±è´¥ï¼ˆå¯èƒ½æ˜¯å›¾ç‰‡æ•°æ®å¤ªå¤§å¯¼è‡´ SQLite æ“ä½œè¶…æ—¶æˆ–å¤±è´¥ï¼‰
            error_type = type(db_error).__name__
            error_detail = str(db_error)
            logger.error(f"ã€Part1 åˆ†æã€‘ä»»åŠ¡åˆ›å»ºå¤±è´¥: {error_type}: {error_detail}", exc_info=True)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®è¿‡å¤§å¯¼è‡´çš„é”™è¯¯
            if "too large" in error_detail.lower() or "exceeded" in error_detail.lower() or "timeout" in error_detail.lower():
                raise error_response(ErrorCode.INTERNAL_ERROR, f"å›¾ç‰‡æ•°æ®è¿‡å¤§ï¼Œæ— æ³•ä¿å­˜åˆ°æ•°æ®åº“ã€‚è¯·å°è¯•ä½¿ç”¨è¾ƒå°çš„å›¾ç‰‡æˆ–è”ç³»ç®¡ç†å‘˜ã€‚")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"æ•°æ®åº“æ“ä½œå¤±è´¥: {error_detail}")

        # è·å– Part1 Prompt æ¨¡æ¿ï¼ˆæ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 23.2 èŠ‚ï¼‰
        try:
            prompt = prompt_template.get_part1_prompt(
                sourceImage, targetImage, exif=None, options={"optional_style": request_data.optional_style}
            )
            logger.info(f"ã€Part1 åˆ†æã€‘Prompt ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
        except Exception as prompt_error:
            # ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ ¸å¼¹çº§è°ƒè¯•ä»£ç å¼€å§‹ ğŸ‘‡ğŸ‘‡ğŸ‘‡
            import traceback
            import sys
            print("\n" + "!"*60)
            print("ğŸ’¥ğŸ’¥ğŸ’¥ åœ¨ get_part1_prompt ä¸­æŠ“åˆ°å‡¶æ‰‹äº†ï¼è¯¦ç»†æŠ¥é”™å¦‚ä¸‹ï¼š")
            print("!"*60)
            traceback.print_exc(file=sys.stdout)  # å¼ºåˆ¶æ‰“å°å †æ ˆåˆ°ç»ˆç«¯
            print("!"*60 + "\n")
            # ğŸ‘†ğŸ‘†ğŸ‘† æ ¸å¼¹çº§è°ƒè¯•ä»£ç ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
            logger.error(f"ã€Part1 åˆ†æã€‘Prompt ç”Ÿæˆå¤±è´¥: {prompt_error}", exc_info=True)
            raise
        
        # ã€è°ƒè¯•ã€‘éªŒè¯ prompt æ˜¯å¦åŒ…å«å…³é”®è¦æ±‚ï¼ˆæ£€æŸ¥ overlays ç›¸å…³è¦æ±‚ï¼‰
        if "reference_overlays" in prompt and "user_overlays" in prompt:
            logger.info(f"ã€Part1 åˆ†æã€‘âœ… Prompt åŒ…å« overlays ä¸¤å¥—åæ ‡è¦æ±‚ï¼ˆreference_overlays å’Œ user_overlaysï¼‰")
        else:
            logger.warning(f"ã€Part1 åˆ†æã€‘âš ï¸ Prompt å¯èƒ½ç¼ºå°‘ overlays ä¸¤å¥—åæ ‡è¦æ±‚ï¼Œè¯·æ£€æŸ¥ prompt æ¨¡æ¿ï¼")
        
        # ã€è°ƒè¯•ã€‘è®°å½• prompt çš„å¼€å¤´å’Œç»“å°¾ï¼ˆç”¨äºéªŒè¯ prompt æ˜¯å¦å®Œæ•´ï¼‰
        logger.debug(f"ã€Part1 åˆ†æã€‘Prompt å¼€å¤´ï¼ˆå‰200å­—ç¬¦ï¼‰: {prompt[:200]}")
        logger.debug(f"ã€Part1 åˆ†æã€‘Prompt ç»“å°¾ï¼ˆå200å­—ç¬¦ï¼‰: {prompt[-200:]}")

        # æ„å»º Gemini API è¯·æ±‚å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
        # ã€é‡è¦ã€‘ä»æ•°æ®åº“è·å–çš„ source_image_data å’Œ target_image_data æ˜¯çº¯ base64 å­—ç¬¦ä¸²ï¼ˆä¸å¸¦ data URL å‰ç¼€ï¼‰
        # å› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ split(",")
        # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨æ¯å¼ å›¾ç‰‡å‰æ·»åŠ æ–‡æœ¬æ ‡è®°ï¼Œæ˜ç¡®æ ‡è¯†å›¾ç‰‡ç±»å‹ï¼Œé˜²æ­¢ Gemini æ··æ·†å›¾ç‰‡é¡ºåº
        contents = [{"role": "user", "parts": [{"text": prompt}]}]
        
        if sourceImage:
            # ã€å¤„ç†ã€‘å¦‚æœ sourceImage æ˜¯ data URL æ ¼å¼ï¼ˆå¦‚ "data:image/jpeg;base64,..."ï¼‰ï¼Œæå– base64 éƒ¨åˆ†
            # å¦‚æœæ˜¯çº¯ base64 å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            source_base64 = sourceImage.split(",")[-1] if "," in sourceImage else sourceImage
            # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨å‚è€ƒå›¾å‰æ·»åŠ æ–‡æœ¬æ ‡è®°ï¼Œæ˜ç¡®æ ‡è¯†è¿™æ˜¯ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆå‚è€ƒå›¾ï¼‰
            contents[0]["parts"].append({
                "text": "âš ï¸âš ï¸âš ï¸ã€å›¾ç‰‡1ï¼šå‚è€ƒå›¾ï¼ˆReference Imageï¼‰ã€‘âš ï¸âš ï¸âš ï¸ è¿™æ˜¯ç¬¬ä¸€å¼ å›¾ç‰‡ï¼Œæ˜¯ç›®æ ‡é£æ ¼å›¾ï¼Œç”¨äºç†è§£ç›®æ ‡è‰²å½©é£æ ¼å’Œæ„å›¾ç‰¹å¾ã€‚æ‰€æœ‰æ„å›¾åˆ†æï¼ˆmodule_2_compositionï¼‰éƒ½å¿…é¡»åŸºäºè¿™å¼ å›¾ç‰‡è¿›è¡Œã€‚è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„é£æ ¼ç‰¹å¾ã€‚âš ï¸âš ï¸âš ï¸ è¿™æ˜¯å‚è€ƒå›¾ï¼Œä¸æ˜¯ç”¨æˆ·å›¾ï¼âš ï¸âš ï¸âš ï¸"
            })
            contents[0]["parts"].append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": source_base64,
                }
            })
            logger.debug(f"ã€Part1 åˆ†æã€‘æºå›¾ï¼ˆå‚è€ƒå›¾ï¼Œç¬¬ä¸€å¼ å›¾ç‰‡ï¼‰æ•°æ®å·²æ·»åŠ åˆ° Gemini è¯·æ±‚ï¼Œbase64 é•¿åº¦: {len(source_base64)} å­—ç¬¦")
        
        if targetImage:
            # ã€å¤„ç†ã€‘å¦‚æœ targetImage æ˜¯ data URL æ ¼å¼ï¼Œæå– base64 éƒ¨åˆ†
            # å¦‚æœæ˜¯çº¯ base64 å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            target_base64 = targetImage.split(",")[-1] if "," in targetImage else targetImage
            # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨ç”¨æˆ·å›¾å‰æ·»åŠ æ–‡æœ¬æ ‡è®°ï¼Œæ˜ç¡®æ ‡è¯†è¿™æ˜¯ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆç”¨æˆ·å›¾ï¼‰
            contents[0]["parts"].append({
                "text": "âš ï¸âš ï¸âš ï¸ã€å›¾ç‰‡2ï¼šç”¨æˆ·å›¾ï¼ˆUser Imageï¼‰ã€‘âš ï¸âš ï¸âš ï¸ è¿™æ˜¯ç¬¬äºŒå¼ å›¾ç‰‡ï¼Œæ˜¯éœ€è¦å¤„ç†çš„å›¾ç‰‡ï¼Œéœ€è¦å‚è€ƒç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆå‚è€ƒå›¾ï¼‰çš„é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸æ˜¯æ„å›¾åˆ†æçš„å¯¹è±¡ã€‚âš ï¸âš ï¸âš ï¸ è¿™æ˜¯ç”¨æˆ·å›¾ï¼Œä¸æ˜¯å‚è€ƒå›¾ï¼âš ï¸âš ï¸âš ï¸"
            })
            contents[0]["parts"].append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": target_base64,
                }
            })
            logger.debug(f"ã€Part1 åˆ†æã€‘ç›®æ ‡å›¾ï¼ˆç”¨æˆ·å›¾ï¼Œç¬¬äºŒå¼ å›¾ç‰‡ï¼‰æ•°æ®å·²æ·»åŠ åˆ° Gemini è¯·æ±‚ï¼Œbase64 é•¿åº¦: {len(target_base64)} å­—ç¬¦")
        
        # ã€æ–¹æ¡ˆ4ï¼šéªŒè¯æ—¥å¿—ã€‘è®°å½•å›¾ç‰‡é¡ºåºçš„è¯¦ç»†ä¿¡æ¯ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜
        logger.info(f"ã€Part1 åˆ†æã€‘å›¾ç‰‡é¡ºåºç¡®è®¤ï¼š")
        logger.info(f"  - ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆå‚è€ƒå›¾ï¼‰ï¼šsourceImage, base64é•¿åº¦={len(source_base64) if sourceImage else 0}")
        logger.info(f"  - ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆç”¨æˆ·å›¾ï¼‰ï¼štargetImage, base64é•¿åº¦={len(target_base64) if targetImage else 0}")
        logger.info(f"  - Gemini API contents parts é¡ºåºï¼š")
        for i, part in enumerate(contents[0]["parts"]):
            if "text" in part:
                part_text = part["text"]
                # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬ï¼Œåªæ˜¾ç¤ºå‰200å­—ç¬¦å’Œå200å­—ç¬¦
                if len(part_text) > 400:
                    text_preview = part_text[:200] + f"... [ä¸­é—´çœç•¥ {len(part_text) - 400} å­—ç¬¦] ..." + part_text[-200:]
                else:
                    text_preview = part_text
                logger.info(f"    Part {i+1}: æ–‡æœ¬ï¼ˆé•¿åº¦={len(part_text)}ï¼Œé¢„è§ˆ={text_preview}ï¼‰")
                # ã€è°ƒè¯•ã€‘éªŒè¯ prompt æ˜¯å¦åŒ…å«å…³é”®è¦æ±‚
                if i == 0:  # ç¬¬ä¸€ä¸ª part æ˜¯ prompt
                    if "reference_overlays" in part_text and "user_overlays" in part_text:
                        logger.info(f"    âœ… Part {i+1} (Prompt) åŒ…å« overlays ä¸¤å¥—åæ ‡è¦æ±‚")
                    else:
                        logger.warning(f"    âš ï¸ Part {i+1} (Prompt) å¯èƒ½ç¼ºå°‘ overlays ä¸¤å¥—åæ ‡è¦æ±‚ï¼")
                    # æ£€æŸ¥ prompt å¼€å¤´æ˜¯å¦åŒ…å«å…³é”®è­¦å‘Š
                    if "ğŸš¨ğŸš¨ğŸš¨ æœ€å…³é”®çš„è¾“å‡ºè¦æ±‚" in part_text[:500]:
                        logger.info(f"    âœ… Part {i+1} (Prompt) å¼€å¤´åŒ…å«å…³é”®è­¦å‘Š")
                    else:
                        logger.warning(f"    âš ï¸ Part {i+1} (Prompt) å¼€å¤´å¯èƒ½ç¼ºå°‘å…³é”®è­¦å‘Šï¼")
            elif "inline_data" in part:
                data_length = len(part["inline_data"]["data"])
                logger.info(f"    Part {i+1}: å›¾ç‰‡ï¼ˆbase64é•¿åº¦={data_length}ï¼‰")

        # ã€ç¬¬ä¸€å±‚ï¼šçœŸç›¸å±‚ - æ‰“å°å®Œæ•´ Promptã€‘åœ¨è°ƒç”¨ Gemini API çš„å‰ä¸€è¡Œï¼Œå¼ºåˆ¶æ‰“å°æœ€ç»ˆç”Ÿæˆçš„å®Œæ•´ Prompt
        # è¿™æ˜¯æ’æŸ¥"å¹½çµè¡Œä¸º"çš„å…³é”®ï¼šç¡®è®¤å®é™…å‘é€ç»™ LLM çš„å†…å®¹
        logger.info("=" * 80)
        logger.info("ã€ç¬¬ä¸€å±‚æ’æŸ¥ï¼šçœŸç›¸å±‚ã€‘========== å®Œæ•´ Prompt å­—ç¬¦ä¸²ï¼ˆå‘é€ç»™ Gemini å‰ï¼‰ ==========")
        logger.info(f"Prompt æ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"Prompt å®Œæ•´å†…å®¹:\n{prompt}")
        logger.info("=" * 80)
        
        # ã€ç¬¬ä¸€å±‚ï¼šçœŸç›¸å±‚ - æ‰“å°å®Œæ•´ Contentsã€‘æ‰“å°æœ€ç»ˆå‘é€ç»™ Gemini API çš„å®Œæ•´ contents
        logger.info("=" * 80)
        logger.info("ã€ç¬¬ä¸€å±‚æ’æŸ¥ï¼šçœŸç›¸å±‚ã€‘========== å®Œæ•´ Contentsï¼ˆå‘é€ç»™ Gemini å‰ï¼‰ ==========")
        logger.info(f"Contents ç»“æ„: {json.dumps(contents, indent=2, ensure_ascii=False)}")
        logger.info("=" * 80)
        
        # ã€Gemini API è°ƒç”¨ã€‘æ·»åŠ è¯¦ç»†çš„æ—¥å¿—å’Œå¼‚å¸¸å¤„ç†
        logger.info(f"ã€Part1 åˆ†æã€‘å‡†å¤‡è°ƒç”¨ Gemini APIï¼Œcontents parts æ•°é‡: {len(contents[0]['parts'])}")
        try:
            gemini_response = gemini_service.generate_text(contents, stage="part1")
            logger.info(f"ã€Part1 åˆ†æã€‘Gemini API è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(gemini_response)} å­—ç¬¦")
        except NameError as name_error:
            # ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ ¸å¼¹çº§è°ƒè¯•ä»£ç å¼€å§‹ ğŸ‘‡ğŸ‘‡ğŸ‘‡
            import traceback
            import sys
            print("\n" + "!"*60)
            print("ğŸ’¥ğŸ’¥ğŸ’¥ åœ¨ Gemini API è°ƒç”¨ä¸­æŠ“åˆ°å‡¶æ‰‹äº†ï¼è¯¦ç»†æŠ¥é”™å¦‚ä¸‹ï¼š")
            print("!"*60)
            traceback.print_exc(file=sys.stdout)  # å¼ºåˆ¶æ‰“å°å †æ ˆåˆ°ç»ˆç«¯
            print("!"*60 + "\n")
            # ğŸ‘†ğŸ‘†ğŸ‘† æ ¸å¼¹çº§è°ƒè¯•ä»£ç ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini API è°ƒç”¨å‘ç”Ÿ NameError: {name_error}", exc_info=True)
            raise
        except TimeoutError as timeout_err:
            # è¶…æ—¶é”™è¯¯ï¼šè®°å½•è¯¦ç»†æ—¥å¿—å¹¶è¿”å›å‹å¥½é”™è¯¯
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini API è°ƒç”¨è¶…æ—¶: {str(timeout_err)}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"AI åˆ†æè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
        except ConnectionError as conn_err:
            # ã€SSL/è¿æ¥é”™è¯¯å¤„ç†ã€‘SSL è¿æ¥é”™è¯¯æˆ–ç½‘ç»œè¿æ¥é”™è¯¯
            error_detail = str(conn_err)
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini API è¿æ¥å¤±è´¥: {error_detail}", exc_info=True)
            
            # ã€å…³é”®ä¿®å¤ã€‘æ˜ç¡®æç¤ºä»£ç†è¿æ¥æ‹’ç»é”™è¯¯
            # [Errno 61] Connection refused é€šå¸¸æ„å‘³ç€ä»£ç†æœåŠ¡å™¨æœªå¯åŠ¨
            if "Connection refused" in error_detail or "Errno 61" in error_detail:
                raise error_response(ErrorCode.INTERNAL_ERROR, "æ— æ³•è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ ClashX(7890) æˆ– Clash Verge(7897) æ˜¯å¦å·²å¯åŠ¨ï¼Œå¹¶ç¡®è®¤ç«¯å£é…ç½®æ­£ç¡®ã€‚")
            
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·æ£€æŸ¥ç½‘ç»œå’Œä»£ç†é…ç½®
            if "SSL" in error_detail or "ssl" in error_detail:
                raise error_response(ErrorCode.INTERNAL_ERROR, "AI åˆ†æå¤±è´¥ï¼šSSL è¿æ¥é”™è¯¯ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– ClashX ä»£ç†é…ç½®ã€‚")
            else:
                raise error_response(ErrorCode.INTERNAL_ERROR, f"AI åˆ†æå¤±è´¥ï¼šç½‘ç»œè¿æ¥é”™è¯¯ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚")
        except RuntimeError as runtime_err:
            # Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–é”™è¯¯
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini æœåŠ¡æœªåˆå§‹åŒ–: {str(runtime_err)}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"AI æœåŠ¡æœªé…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
        except TimeoutError as timeout_err:
            # ã€è¶…æ—¶é”™è¯¯å¤„ç†ã€‘è¶…æ—¶é”™è¯¯å·²åœ¨ gemini_service ä¸­å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯è½¬å‘
            error_detail = str(timeout_err)
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini API è°ƒç”¨è¶…æ—¶: {error_detail}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"AI åˆ†æè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
        except Exception as gemini_err:
            # å…¶ä»– Gemini API è°ƒç”¨é”™è¯¯ï¼ˆåŒ…æ‹¬é‡è¯•åä»ç„¶å¤±è´¥çš„ç½‘ç»œé”™è¯¯ï¼‰
            error_type = type(gemini_err).__name__
            error_detail = str(gemini_err)
            logger.error(f"ã€Part1 åˆ†æã€‘Gemini API è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•ï¼‰: {error_type}: {error_detail}", exc_info=True)
            
            # ã€SSL é”™è¯¯ç‰¹æ®Šå¤„ç†ã€‘æ£€æµ‹ SSL ç›¸å…³é”™è¯¯
            if "SSL" in error_detail or "ssl" in error_detail or "UNEXPECTED_EOF" in error_detail:
                raise error_response(ErrorCode.INTERNAL_ERROR, "AI åˆ†æå¤±è´¥ï¼šSSL è¿æ¥é”™è¯¯ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– ClashX ä»£ç†é…ç½®ã€‚")
            
            # ã€ç½‘ç»œè¿æ¥é”™è¯¯ç‰¹æ®Šå¤„ç†ã€‘æ£€æµ‹ç½‘ç»œè¿æ¥é”™è¯¯ï¼ˆé‡è¯•åä»ç„¶å¤±è´¥ï¼‰
            if "Server disconnected" in error_detail or "Connection" in error_type or "RemoteProtocolError" in error_type:
                raise error_response(ErrorCode.INTERNAL_ERROR, "AI åˆ†æå¤±è´¥ï¼šç½‘ç»œè¿æ¥ä¸­æ–­ã€‚å·²è‡ªåŠ¨é‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚")
            
            raise error_response(ErrorCode.INTERNAL_ERROR, f"AI åˆ†æå¤±è´¥: {error_detail}")
        
        # ã€æ­¥éª¤2ï¼šJSON è§£ææ¸…æ´—é€»è¾‘ã€‘æ‰“å° Gemini åŸå§‹è¾“å‡ºï¼ˆå®Œæ•´ RAW OUTPUTï¼‰
        # âš ï¸ å…³é”®è°ƒè¯•ï¼šæ‰“å°æœ€åŸå§‹çš„è¿”å›ï¼Œçœ‹çœ‹ AI åˆ°åº•è¯´äº†ä»€ä¹ˆ
        logger.info("=" * 80)
        logger.info("========== GEMINI RAW OUTPUT START ==========")
        logger.info(f"Part1 Gemini åŸå§‹å“åº”é•¿åº¦: {len(gemini_response)} å­—ç¬¦")
        # æ‰“å°å®Œæ•´åŸå§‹è¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼‰
        logger.info(f"Part1 Gemini å®Œæ•´åŸå§‹å“åº”:\n{gemini_response}")
        logger.info("========== GEMINI RAW OUTPUT END ==========")
        logger.info("=" * 80)
        
        # ã€é‡è¦ã€‘å°†å®Œæ•´çš„ Gemini å“åº”ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¾¿äºè°ƒè¯•å’ŒæŸ¥çœ‹
        # æ–‡ä»¶è·¯å¾„ï¼š/tmp/gemini_response_part1_<timestamp>.json
        timestamp = int(time.time())
        gemini_response_file = f"/tmp/gemini_response_part1_{timestamp}.json"
        try:
            with open(gemini_response_file, 'w', encoding='utf-8') as f:
                f.write(gemini_response)
            logger.info(f"Part1 Gemini å®Œæ•´å“åº”å·²ä¿å­˜åˆ°: {gemini_response_file}")
        except Exception as save_error:
            logger.warning(f"ä¿å­˜ Gemini å“åº”åˆ°æ–‡ä»¶å¤±è´¥: {save_error}")

        # ã€æ­¥éª¤2ï¼šJSON è§£ææ¸…æ´—é€»è¾‘ã€‘ä½¿ç”¨ clean_json_response æ¸…æ´— JSON å“åº”
        # é˜²æ­¢ Markdown ä»£ç å—æ ‡è®°å¹²æ‰° JSON è§£æ
        from ..services.prompt_template import clean_json_response
        cleaned_response = clean_json_response(gemini_response)
        logger.info(f"Part1 Gemini JSON æ¸…æ´—åé•¿åº¦: {len(cleaned_response)} å­—ç¬¦")
        
        # ã€æ­¥éª¤2ï¼šéªŒè¯ã€‘æ£€æŸ¥æ¸…æ´—åçš„ JSON æ˜¯å¦åŒ…å«æ–°å­—æ®µ
        if "spatial_analysis" in cleaned_response:
            logger.info("âœ… æ¸…æ´—åçš„ JSON åŒ…å« 'spatial_analysis' å­—æ®µ")
        else:
            logger.warning("âš ï¸ æ¸…æ´—åçš„ JSON ä¸åŒ…å« 'spatial_analysis' å­—æ®µ")
        
        if "ref_visual_subject_box" in cleaned_response:
            logger.info("âœ… æ¸…æ´—åçš„ JSON åŒ…å« 'ref_visual_subject_box' å­—æ®µï¼ˆç ´åæ€§å‘½åï¼‰")
        else:
            logger.warning("âš ï¸ æ¸…æ´—åçš„ JSON ä¸åŒ…å« 'ref_visual_subject_box' å­—æ®µ")
        
        if "ref_visual_mass_polygon" in cleaned_response:
            logger.info("âœ… æ¸…æ´—åçš„ JSON åŒ…å« 'ref_visual_mass_polygon' å­—æ®µ")
        else:
            logger.warning("âš ï¸ æ¸…æ´—åçš„ JSON ä¸åŒ…å« 'ref_visual_mass_polygon' å­—æ®µ")

        try:
            gemini_json = json.loads(cleaned_response)
            logger.info(f"Part1 Gemini JSON è§£ææˆåŠŸ: ç±»å‹ = {type(gemini_json)}")
        except NameError as name_error:
            # ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ ¸å¼¹çº§è°ƒè¯•ä»£ç å¼€å§‹ ğŸ‘‡ğŸ‘‡ğŸ‘‡
            import traceback
            import sys
            print("\n" + "!"*60)
            print("ğŸ’¥ğŸ’¥ğŸ’¥ åœ¨ JSON è§£æä¸­æŠ“åˆ°å‡¶æ‰‹äº†ï¼è¯¦ç»†æŠ¥é”™å¦‚ä¸‹ï¼š")
            print("!"*60)
            traceback.print_exc(file=sys.stdout)  # å¼ºåˆ¶æ‰“å°å †æ ˆåˆ°ç»ˆç«¯
            print("!"*60 + "\n")
            # ğŸ‘†ğŸ‘†ğŸ‘† æ ¸å¼¹çº§è°ƒè¯•ä»£ç ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
            logger.error(f"ã€Part1 åˆ†æã€‘JSON è§£æå‘ç”Ÿ NameError: {name_error}", exc_info=True)
            raise
            if isinstance(gemini_json, dict):
                logger.info(f"Part1 Gemini JSON æ˜¯å­—å…¸ï¼Œkeys = {list(gemini_json.keys())}")
            elif isinstance(gemini_json, list):
                logger.info(f"Part1 Gemini JSON æ˜¯æ•°ç»„ï¼Œé•¿åº¦ = {len(gemini_json)}")
                if len(gemini_json) > 0:
                    logger.info(f"Part1 Gemini JSON æ•°ç»„ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹ = {type(gemini_json[0])}")
                    if isinstance(gemini_json[0], dict):
                        logger.info(f"Part1 Gemini JSON æ•°ç»„ç¬¬ä¸€ä¸ªå…ƒç´  keys = {list(gemini_json[0].keys())}")
        except Exception as parse_error:
            logger.warning(f"Part1 Gemini JSON è§£æå¤±è´¥: {parse_error}, å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–")
            import re
            json_match = re.search(r'\{.*\}', gemini_response, re.DOTALL)
            if json_match:
                try:
                    gemini_json = json.loads(json_match.group())
                    logger.info(f"Part1 Gemini JSON æ­£åˆ™æå–æˆåŠŸ: ç±»å‹ = {type(gemini_json)}")
                except Exception as regex_error:
                    logger.error(f"Part1 Gemini JSON æ­£åˆ™æå–ä¹Ÿå¤±è´¥: {regex_error}")
                    raise ValueError("æ— æ³•è§£æ Gemini è¿”å›çš„ JSON")
            else:
                logger.error("Part1 Gemini å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼çš„æ•°æ®")
                raise ValueError("æ— æ³•è§£æ Gemini è¿”å›çš„ JSON")

        # ã€æ–°å¢ã€‘ç”Ÿæˆæ˜¾è‘—æ€§é®ç½©å›¾ï¼ˆç”¨äºå‰ç«¯ Visual Mass åŠŸèƒ½ï¼‰
        # æ³¨æ„ï¼šé®ç½©å›¾åŸºäºå‚è€ƒå›¾ï¼ˆsourceImageï¼‰ç”Ÿæˆï¼Œå› ä¸ºæ„å›¾åˆ†æéƒ½æ˜¯é’ˆå¯¹å‚è€ƒå›¾çš„
        # ã€é‡è¦ã€‘æ˜¾è‘—æ€§æ£€æµ‹å¯èƒ½è€—æ—¶è¾ƒé•¿ï¼Œä½¿ç”¨å¼‚æ­¥ä»»åŠ¡æˆ–è¶…æ—¶æ§åˆ¶ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
        saliency_mask_url = None
        if sourceImage:
            try:
                logger.info(f"ã€Part1 åˆ†æã€‘å¼€å§‹ç”Ÿæˆæ˜¾è‘—æ€§é®ç½©å›¾ï¼ˆåŸºäºå‚è€ƒå›¾ï¼‰...")
                saliency_start_time = time.time()
                # ã€è¶…æ—¶æ§åˆ¶ã€‘æ˜¾è‘—æ€§æ£€æµ‹å¯èƒ½è€—æ—¶è¾ƒé•¿ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆ30ç§’ï¼‰
                # å¦‚æœè¶…æ—¶ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­ä½¿ç”¨å¤šè¾¹å½¢æ–¹æ¡ˆ
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ˜¾è‘—æ€§æ£€æµ‹ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
                def _generate_saliency():
                    try:
                        return saliency_service.generate_saliency_mask(
                            image_data=sourceImage,
                            task_id=task.id if task else None,
                            user_id=current_user.id
                        )
                    except Exception as e:
                        logger.error(f"æ˜¾è‘—æ€§æ£€æµ‹çº¿ç¨‹å¼‚å¸¸: {type(e).__name__}: {str(e)}", exc_info=True)
                        return None
                
                # ä½¿ç”¨ ThreadPoolExecutor æ‰§è¡Œæ˜¾è‘—æ€§æ£€æµ‹ï¼Œè®¾ç½®è¶…æ—¶
                # ã€æ³¨æ„ã€‘concurrent.futures å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥
                saliency_executor = ThreadPoolExecutor(max_workers=1)
                saliency_future = saliency_executor.submit(_generate_saliency)
                
                try:
                    # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼š30ç§’ï¼ˆæ˜¾è‘—æ€§æ£€æµ‹ä¸åº”è¯¥å ç”¨å¤ªé•¿æ—¶é—´ï¼‰
                    saliency_mask_url = saliency_future.result(timeout=30.0)
                    saliency_elapsed = time.time() - saliency_start_time
                    if saliency_mask_url:
                        logger.info(f"ã€Part1 åˆ†æã€‘æ˜¾è‘—æ€§é®ç½©å›¾ç”ŸæˆæˆåŠŸ: {saliency_mask_url}ï¼Œè€—æ—¶: {saliency_elapsed:.2f}ç§’")
                    else:
                        logger.warning(f"ã€Part1 åˆ†æã€‘æ˜¾è‘—æ€§é®ç½©å›¾ç”Ÿæˆå¤±è´¥ï¼ˆè¿”å› Noneï¼‰ï¼Œè€—æ—¶: {saliency_elapsed:.2f}ç§’ï¼Œå°†ä½¿ç”¨å¤šè¾¹å½¢æ–¹æ¡ˆï¼ˆvisual_mass.verticesï¼‰")
                except FutureTimeoutError:
                    saliency_elapsed = time.time() - saliency_start_time
                    logger.warning(f"ã€Part1 åˆ†æã€‘æ˜¾è‘—æ€§é®ç½©å›¾ç”Ÿæˆè¶…æ—¶ï¼ˆè¶…è¿‡ 30 ç§’ï¼‰ï¼Œè€—æ—¶: {saliency_elapsed:.2f}ç§’ï¼Œå°†ä½¿ç”¨å¤šè¾¹å½¢æ–¹æ¡ˆï¼ˆvisual_mass.verticesï¼‰")
                    # å–æ¶ˆä»»åŠ¡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    saliency_future.cancel()
                finally:
                    saliency_executor.shutdown(wait=False)  # ä¸ç­‰å¾…ï¼Œç«‹å³å…³é—­çº¿ç¨‹æ± 
                    
            except Exception as saliency_err:
                # æ˜¾è‘—æ€§æ£€æµ‹å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œè®°å½•è­¦å‘Šå³å¯
                error_type = type(saliency_err).__name__
                error_detail = str(saliency_err)
                logger.warning(f"ã€Part1 åˆ†æã€‘æ˜¾è‘—æ€§é®ç½©å›¾ç”Ÿæˆå¤±è´¥: {error_type}: {error_detail}ï¼Œå°†ä½¿ç”¨å¤šè¾¹å½¢æ–¹æ¡ˆï¼ˆvisual_mass.verticesï¼‰")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œä¸»æµç¨‹
        
        # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•ä¼ é€’ç»™ format_part1 çš„æ•°æ®ç±»å‹å’Œå†…å®¹
        logger.info(f"Part1 è°ƒç”¨ format_part1: gemini_json ç±»å‹ = {type(gemini_json)}, feasibility_result ç±»å‹ = {type(feasibility_result)}")
        if isinstance(gemini_json, dict):
            logger.info(f"Part1 gemini_json keys = {list(gemini_json.keys())}")
            # æ£€æŸ¥æ˜¯å¦åŒ…å« professional_evaluation å’Œ composition
            if "professional_evaluation" in gemini_json:
                pe = gemini_json.get("professional_evaluation", {})
                logger.info(f"Part1 gemini_json åŒ…å« professional_evaluation: ç±»å‹ = {type(pe)}, keys = {list(pe.keys()) if isinstance(pe, dict) else 'not dict'}")
            else:
                logger.warning("Part1 gemini_json ä¸åŒ…å« professional_evaluation å­—æ®µï¼")
            if "composition" in gemini_json:
                comp = gemini_json.get("composition", {})
                logger.info(f"Part1 gemini_json åŒ…å« composition: ç±»å‹ = {type(comp)}, keys = {list(comp.keys()) if isinstance(comp, dict) else 'not dict'}")
            else:
                logger.warning("Part1 gemini_json ä¸åŒ…å« composition å­—æ®µï¼")
        if feasibility_result:
            logger.debug(f"Part1 feasibility_result keys = {list(feasibility_result.keys()) if isinstance(feasibility_result, dict) else 'not dict'}")

        # è°ƒç”¨ format_part1 æ ¼å¼åŒ–æ•°æ®ï¼ˆæ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å³ä½¿æ ¼å¼åŒ–å¤±è´¥ä¹Ÿèƒ½è¿”å›é”™è¯¯ç»“æ„ï¼‰
        # ã€æ–°å¢ã€‘ä¼ é€’æ˜¾è‘—æ€§é®ç½©å›¾ URL åˆ° formatter
        try:
            structured_result = formatter.format_part1(gemini_json, feasibility_result, saliency_mask_url=saliency_mask_url)
        except NameError as name_error:
            # ã€å…³é”®ä¿®å¤ã€‘æ•è· NameErrorï¼Œè®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å˜é‡åå’Œå †æ ˆè·Ÿè¸ª
            error_detail = str(name_error)
            import traceback
            tb_str = ''.join(traceback.format_exception(type(name_error), name_error, name_error.__traceback__))
            logger.error(f"Part1 æ ¼å¼åŒ–è¿‡ç¨‹å‘ç”Ÿ NameError: {error_detail}", exc_info=True)
            logger.error(f"Part1 NameError å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{tb_str}")
            logger.error(f"Part1 NameError è¯¦ç»†ä¿¡æ¯: é”™è¯¯ç±»å‹={type(name_error).__name__}, é”™è¯¯æ¶ˆæ¯={error_detail}")
            # å¦‚æœæ˜¯ 'x' is not defined é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
            if "'x' is not defined" in error_detail or "name 'x'" in error_detail:
                logger.error("Part1 NameError è¯Šæ–­: æ£€æµ‹åˆ° 'x' å˜é‡æœªå®šä¹‰é”™è¯¯ï¼Œå¯èƒ½å‘ç”Ÿåœ¨åæ ‡å¤„ç†é€»è¾‘ä¸­")
                logger.error("Part1 NameError è¯Šæ–­: è¯·æ£€æŸ¥ analysis_formatter.py ä¸­çš„ validate_and_fix_coords å‡½æ•°å’Œæ‰€æœ‰åæ ‡å¤„ç†é€»è¾‘")
                # å°è¯•ä»å †æ ˆè·Ÿè¸ªä¸­æå–æ–‡ä»¶åå’Œè¡Œå·
                if hasattr(name_error, '__traceback__') and name_error.__traceback__:
                    tb = name_error.__traceback__
                    frame_count = 0
                    while tb and frame_count < 20:  # é™åˆ¶æœ€å¤š20å±‚ï¼Œé¿å…æ— é™å¾ªç¯
                        filename = tb.tb_frame.f_code.co_filename
                        lineno = tb.tb_lineno
                        func_name = tb.tb_frame.f_code.co_name
                        # è¯»å–è¯¥è¡Œçš„ä»£ç 
                        try:
                            with open(filename, 'r', encoding='utf-8') as f:
                                code_lines = f.readlines()
                                if lineno <= len(code_lines):
                                    code_line = code_lines[lineno - 1].strip()
                                    logger.error(f"Part1 NameError ä½ç½® {frame_count}: æ–‡ä»¶={filename}, è¡Œå·={lineno}, å‡½æ•°={func_name}, ä»£ç ={code_line}")
                        except Exception:
                            logger.error(f"Part1 NameError ä½ç½® {frame_count}: æ–‡ä»¶={filename}, è¡Œå·={lineno}, å‡½æ•°={func_name}")
                        tb = tb.tb_next
                        frame_count += 1
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚å¤„ç†
        except Exception as format_error:
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯å¹¶è¿”å›é”™è¯¯ç»“æ„
            logger.error(f"Part1 æ ¼å¼åŒ–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {format_error}", exc_info=True)
            # format_part1 å†…éƒ¨å·²ç»æœ‰å¼‚å¸¸å¤„ç†ï¼Œä½†å¦‚æœä»ç„¶æŠ›å‡ºå¼‚å¸¸ï¼Œè¯´æ˜æ˜¯ä¸¥é‡é”™è¯¯
            # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯ç»“æ„ï¼Œç¡®ä¿æ¥å£èƒ½æ­£å¸¸è¿”å›
            structured_result = {
                "protocolVersion": "2025-02",
                "stage": "part1",
                "meta": {
                    "warnings": [f"æ ¼å¼åŒ–å¤±è´¥: {str(format_error)}"],
                    "rawNaturalLanguage": "",
                },
                "sections": {
                    "photoReview": {
                        "naturalLanguage": {},
                        "structured": {
                            "overviewSummary": "",
                            "dimensions": {},
                            "photographerStyleSummary": "",
                        },
                    },
                    "composition": {
                        "naturalLanguage": {},
                        "structured": {
                            "advanced_sections": {},
                        },
                    },
                    "lighting": {
                        "naturalLanguage": {},
                        "structured": {},
                    },
                    "color": {
                        "naturalLanguage": {},
                        "structured": {},
                    },
                },
            }
        
        # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æ ¼å¼åŒ–åçš„ç»“æœ
        logger.info(f"Part1 æ ¼å¼åŒ–å®Œæˆ: structured_result keys = {list(structured_result.keys()) if isinstance(structured_result, dict) else 'not dict'}")
        if isinstance(structured_result, dict) and "sections" in structured_result:
            sections = structured_result.get("sections", {})
            logger.info(f"Part1 sections keys = {list(sections.keys())}")
            if "photoReview" in sections:
                photo_review = sections.get("photoReview", {})
                logger.info(f"Part1 photoReview keys = {list(photo_review.keys())}")
                if "structured" in photo_review:
                    structured_data = photo_review.get("structured", {})
                    logger.info(f"Part1 photoReview.structured keys = {list(structured_data.keys())}")
                    # ã€æ–°å¢ã€‘è®°å½•å…³é”®å­—æ®µ
                    logger.info(f"Part1 style_summary é•¿åº¦ = {len(structured_data.get('style_summary', ''))} å­—ç¬¦")
                    logger.info(f"Part1 comprehensive_review é•¿åº¦ = {len(structured_data.get('comprehensive_review', ''))} å­—ç¬¦")
                    logger.debug(f"Part1 overviewSummary = {structured_data.get('overviewSummary', 'empty')[:100] if structured_data.get('overviewSummary') else 'empty'}...")
                    # ã€æ–°å¢ã€‘è®°å½• simulated_histogram_data
                    histogram_data = structured_data.get('simulated_histogram_data')
                    if histogram_data:
                        logger.info(f"Part1 simulated_histogram_data å­˜åœ¨, keys = {list(histogram_data.keys()) if isinstance(histogram_data, dict) else 'not dict'}")
                    else:
                        logger.warning(f"Part1 simulated_histogram_data ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                    # ã€æ–°å¢ã€‘è®°å½• overlays æ•°æ®ï¼ˆç”¨äºå‰ç«¯å›¾ç‰‡é«˜äº®æ˜¾ç¤ºï¼‰
                    overlays = structured_data.get('overlays', {})
                    if overlays and isinstance(overlays, dict):
                        logger.info(f"Part1 overlays keys = {list(overlays.keys())}")
                        for key, value in overlays.items():
                            if isinstance(value, dict):
                                logger.debug(f"Part1 overlays.{key} = {{x: {value.get('x', 'N/A')}, y: {value.get('y', 'N/A')}, w: {value.get('w', 'N/A')}, h: {value.get('h', 'N/A')}, label: {value.get('label', 'N/A')}}}")
                    else:
                        logger.warning(f"Part1 overlays ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œç±»å‹ = {type(overlays)}")
                        logger.warning(f"Part1 âš ï¸ overlays æ•°æ®ç¼ºå¤±ï¼Œå‰ç«¯å°†æ— æ³•æ˜¾ç¤ºå›¾ç‰‡åŒºåŸŸé«˜äº®åŠŸèƒ½")
            if "composition" in sections:
                composition = sections.get("composition", {})
                logger.info(f"Part1 composition keys = {list(composition.keys())}")
                if "structured" in composition:
                    comp_structured = composition.get("structured", {})
                    logger.info(f"Part1 composition.structured keys = {list(comp_structured.keys())}")
                    if "advanced_sections" in comp_structured:
                        adv_sections = comp_structured.get("advanced_sections", {})
                        logger.info(f"Part1 advanced_sections keys = {list(adv_sections.keys()) if isinstance(adv_sections, dict) else 'not dict'}")

        # æ›´æ–°ä»»åŠ¡ Part1 ç»“æœï¼ˆæ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿æ•°æ®åº“æ“ä½œå¤±è´¥æ—¶èƒ½æ­£ç¡®è¿”å›é”™è¯¯ï¼‰
        try:
            task_service.update_task_part1(
                db,
                task.id,
                gemini_json,
                structured_result,
                gemini_response,
                structured_result.get("sections", {}).get("photoReview", {}).get("structured", {}).get("overviewSummary", ""),
                json.dumps(structured_result.get("workflow_draft", {})),
                feasibility_result,
            )
            logger.info(f"ã€Part1 åˆ†æã€‘ä»»åŠ¡ Part1 ç»“æœæ›´æ–°æˆåŠŸ: taskId={task.id}")
        except Exception as db_error:
            # æ•°æ®åº“æ“ä½œå¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ•°æ®å¤ªå¤§å¯¼è‡´ SQLite æ“ä½œè¶…æ—¶æˆ–å¤±è´¥ï¼‰
            error_type = type(db_error).__name__
            error_detail = str(db_error)
            logger.error(f"ã€Part1 åˆ†æã€‘ä»»åŠ¡ Part1 ç»“æœæ›´æ–°å¤±è´¥: {error_type}: {error_detail}", exc_info=True)
            # å³ä½¿æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œä¹Ÿå°è¯•è¿”å›åˆ†æç»“æœï¼ˆå› ä¸ºåˆ†æå·²ç»å®Œæˆï¼‰
            # ä½†è®°å½•è­¦å‘Šï¼Œæç¤ºç”¨æˆ·ç»“æœå¯èƒ½æœªä¿å­˜
            logger.warning(f"ã€Part1 åˆ†æã€‘âš ï¸ åˆ†æç»“æœå·²ç”Ÿæˆï¼Œä½†ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼Œå°†è¿”å›ç»“æœä½†ä¸ä¿å­˜")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è¿”å›ç»“æœ

        return success_response(
            data={
                "taskId": task.id,
                "stage": "part1",
                "status": "part1_completed",
                "structuredAnalysis": structured_result,
                "naturalLanguage": gemini_response,
                "protocolVersion": "2025-02",
            },
        )
    except HTTPException:
        raise
    except Exception as e:
        # ã€å¢å¼ºé”™è¯¯å¤„ç†ã€‘è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬é”™è¯¯ç±»å‹ã€é”™è¯¯æ¶ˆæ¯ã€å †æ ˆè·Ÿè¸ª
        error_type = type(e).__name__
        error_detail = str(e)
        logger.error(f"ã€Part1 åˆ†æã€‘âŒ åˆ†æå¤±è´¥: {error_type}: {error_detail}", exc_info=True)
        logger.error(f"ã€Part1 åˆ†æã€‘è¯·æ±‚è·¯å¾„: /api/analyze/part1")
        logger.error(f"ã€Part1 åˆ†æã€‘uploadId: {request_data.uploadId if hasattr(request_data, 'uploadId') else 'unknown'}")
        raise error_response(ErrorCode.INTERNAL_ERROR, f"Part1 åˆ†æå¤±è´¥: {error_detail}")


@router.post("/part2")
async def analyze_part2(
    request_data: Part2RequestSchema = Body(...),
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """
    Part2 åˆ†ææ¥å£ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
    
    æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 16 èŠ‚å’Œç¬¬ 793 è¡Œï¼ŒPart2 æ¥å£åº”ç«‹å³è¿”å› { status: 'processing' }ï¼Œ
    å®é™…çš„ Gemini è°ƒç”¨å’Œæ•°æ®åº“æ›´æ–°åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œå‰ç«¯é€šè¿‡è½®è¯¢è·å–ç»“æœã€‚
    
    ã€é‡è¦ã€‘å‚æ•°æ ¼å¼ï¼š
        - è¯·æ±‚ä½“æ ¼å¼ï¼šJSON body { "taskId": "uuid" }
        - æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 793 è¡Œï¼šæ¥å£ï¼šPOST /api/analyze/part2ï¼Œè¯·æ±‚ä½“ { taskId }
        - å‰ç«¯å‘é€çš„æ˜¯ JSON æ ¼å¼ï¼šbody: JSON.stringify({ taskId })
    
    Args:
        request_data: è¯·æ±‚æ•°æ®ï¼ˆJSON bodyï¼‰
            {
                "taskId": str  # ä»»åŠ¡ IDï¼ˆä» Part1 è¿”å›ï¼Œå¿…å¡«ï¼‰
            }
        credentials: JWT Tokenï¼ˆBearerï¼‰
        db: æ•°æ®åº“ä¼šè¯
        
    Returns:
        {
            "code": 0,
            "message": "ok",
            "data": {
                "taskId": "uuid",
                "stage": "part2",
                "status": "processing"  # ç«‹å³è¿”å› processing çŠ¶æ€
            }
        }
        
    Note:
        - åå°ä»»åŠ¡ä¼šæ‰§è¡Œ Gemini API è°ƒç”¨ã€æ•°æ®æ ¼å¼åŒ–ã€æ•°æ®åº“æ›´æ–°
        - å‰ç«¯éœ€è¦é€šè¿‡ GET /api/analyze/{taskId} è½®è¯¢è·å–æœ€ç»ˆç»“æœ
        - è½®è¯¢é—´éš”å»ºè®® 3 ç§’ï¼Œæœ€å¤§è½®è¯¢æ—¶é•¿ 2 åˆ†é’Ÿ
    """
    # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•è¯·æ±‚æ¥æ”¶æ—¶é—´ï¼Œç”¨äºè¿½è¸ªè¯·æ±‚å¤„ç†æ—¶é—´
    request_start_time = time.time()
    taskId = request_data.taskId  # ä»è¯·æ±‚æ•°æ®ä¸­æå– taskId
    logger.info(f"ã€Part2 è¯·æ±‚å¼€å§‹ã€‘taskId={taskId}, æ—¶é—´æˆ³={request_start_time}")
    logger.info(f"ã€Part2 è¯·æ±‚æ•°æ®ã€‘request_data={request_data.model_dump()}")
    try:
        # 1. éªŒè¯ç”¨æˆ·èº«ä»½
        current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)
        logger.info(f"ã€Part2 ç”¨æˆ·éªŒè¯å®Œæˆã€‘userId={current_user.id}, taskId={taskId}, è€—æ—¶={time.time() - request_start_time:.2f}ç§’")
        
        # 2. è·å–ä»»åŠ¡ä¿¡æ¯
        task = task_service.get_task(db, taskId)
        if not task:
            raise error_response(ErrorCode.TASK_NOT_FOUND, "ä»»åŠ¡ä¸å­˜åœ¨")

        if task.user_id != current_user.id:
            raise error_response(ErrorCode.FORBIDDEN, "æ— æƒè®¿é—®æ­¤ä»»åŠ¡")

        # 3. æ£€æŸ¥ç”¨é‡é™åˆ¶ï¼ˆä¸¥æ ¼é™æµï¼Œè¶…å‡ºåˆ™è¿”å›é”™è¯¯ç ï¼‰
        # æ³¨æ„ï¼šç®¡ç†å‘˜è´¦å·ä¸å—ç”¨é‡é™åˆ¶ï¼ˆæ ¹æ®å¼€å‘æ–¹æ¡ˆï¼Œç®¡ç†å‘˜æ‹¥æœ‰æ‰€æœ‰æƒé™ï¼‰
        allowed, error_code = usage_service.check_usage_limit(db, current_user.id, "analysis", user_role=current_user.role)
        if not allowed:
            raise error_response(error_code, "åˆ†ææ¬¡æ•°å·²è¾¾ä¸Šé™")

        # 4. ç«‹å³è¿”å› processing çŠ¶æ€ï¼Œå¹¶åœ¨åå°æ‰§è¡Œå®é™…åˆ†æ
        # æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 16 èŠ‚ï¼ŒPart2 æ¥å£åº”ç«‹å³è¿”å› { status: 'processing' }
        # å®é™…çš„ Gemini è°ƒç”¨å’Œæ•°æ®åº“æ›´æ–°åœ¨åå°å¼‚æ­¥æ‰§è¡Œ
        asyncio.create_task(
            _run_part2_analysis_job(
                task_id=taskId,
                user_id=current_user.id,
                db_session=db,
            )
        )
        request_elapsed_time = time.time() - request_start_time
        logger.info(f"ã€Part2 ä»»åŠ¡å·²æäº¤åå°ã€‘taskId={taskId}, è¯·æ±‚å¤„ç†è€—æ—¶={request_elapsed_time:.2f}ç§’, å³å°†è¿”å›å“åº”")
        response_data = success_response(data={"taskId": taskId, "stage": "part2", "status": "processing"})
        logger.info(f"ã€Part2 è¯·æ±‚å®Œæˆã€‘taskId={taskId}, æ€»è€—æ—¶={time.time() - request_start_time:.2f}ç§’, å“åº”çŠ¶æ€=processing")
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ã€Part2 è¯·æ±‚å¤±è´¥ã€‘taskId={taskId}, é”™è¯¯: {e}", exc_info=True)
        raise error_response(ErrorCode.INTERNAL_ERROR, f"Part2 åˆ†æè¯·æ±‚å¤±è´¥: {str(e)}")


async def _run_part2_analysis_job(task_id: str, user_id: int, db_session: Session):
    """
    åå°æ‰§è¡Œ Part2 åˆ†æä»»åŠ¡çš„å®é™…é€»è¾‘
    
    æ ¹æ®å¼€å‘æ–¹æ¡ˆç¬¬ 16 èŠ‚ï¼Œæ­¤å‡½æ•°åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼ŒåŒ…æ‹¬ï¼š
    1. è°ƒç”¨ Gemini API è·å– Part2 åˆ†æç»“æœ
    2. æ ¼å¼åŒ–æ•°æ®ï¼ˆä½¿ç”¨ analysis_formatterï¼‰
    3. æ›´æ–°æ•°æ®åº“ï¼ˆtask_service.update_task_part2ï¼‰
    4. æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º completed æˆ– failed
    
    Args:
        task_id: ä»»åŠ¡ ID
        user_id: ç”¨æˆ· IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        db_session: æ•°æ®åº“ä¼šè¯ï¼ˆæ³¨æ„ï¼šåå°ä»»åŠ¡éœ€è¦ä½¿ç”¨æ–°çš„æ•°æ®åº“ä¼šè¯ï¼‰
        
    Note:
        - æ­¤å‡½æ•°åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸ä¼šé˜»å¡å‰ç«¯è¯·æ±‚
        - å¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œä¼šå°†ä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸º failedï¼Œå¹¶è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        - å‰ç«¯é€šè¿‡è½®è¯¢ GET /api/analyze/{taskId} è·å–æœ€ç»ˆç»“æœ
    """
    # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•åå°ä»»åŠ¡å¼€å§‹æ—¶é—´
    job_start_time = time.time()
    logger.info(f"ã€Part2 åå°ä»»åŠ¡å¼€å§‹ã€‘taskId={task_id}, æ—¶é—´æˆ³={job_start_time}, userId={user_id}")
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®åº“ä¼šè¯ï¼Œå› ä¸ºåå°ä»»åŠ¡åœ¨ä¸åŒçš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
    # å¹¶ä¸” db_session æ˜¯é€šè¿‡ Depends æ³¨å…¥çš„ï¼Œä¸èƒ½ç›´æ¥åœ¨åå°ä»»åŠ¡ä¸­é‡ç”¨
    db: Session = next(get_db())
    try:
        # 1. è·å–ä»»åŠ¡ä¿¡æ¯
        task = task_service.get_task(db, task_id)
        if not task:
            error_msg = f"ä»»åŠ¡ä¸å­˜åœ¨: taskId={task_id}"
            logger.error(f"ã€Part2 åå°ä»»åŠ¡å¤±è´¥ã€‘{error_msg}")
            # å¦‚æœä»»åŠ¡ä¸å­˜åœ¨ï¼Œå°è¯•æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆè™½ç„¶ä»»åŠ¡ä¸å­˜åœ¨ï¼Œä½†ä¸ºäº†å‰ç«¯èƒ½è·å–åˆ°é”™è¯¯ä¿¡æ¯ï¼‰
            db_for_error: Session = next(get_db())
            try:
                task_service.update_task_status(db_for_error, task_id, "failed", error_msg)
                logger.info(f"ã€Part2 ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºå¤±è´¥ï¼ˆä»»åŠ¡ä¸å­˜åœ¨ï¼‰ã€‘taskId={task_id}")
            except Exception:
                pass  # å¦‚æœä»»åŠ¡ä¸å­˜åœ¨ï¼Œæ›´æ–°çŠ¶æ€ä¹Ÿä¼šå¤±è´¥ï¼Œå¿½ç•¥æ­¤é”™è¯¯
            finally:
                db_for_error.close()
            return
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        logger.info(f"ã€Part2 åå°ä»»åŠ¡ã€‘ä»»åŠ¡ä¿¡æ¯: taskId={task_id}, userId={task.user_id}, å½“å‰çŠ¶æ€={task.status}, æ˜¯å¦æœ‰æºå›¾={bool(task.source_image_data)}, æ˜¯å¦æœ‰ç›®æ ‡å›¾={bool(task.target_image_data)}")
        
        # 2. å°†ä»»åŠ¡çŠ¶æ€è®¾ç½®ä¸º processingï¼ˆè¡¨ç¤ºæ­£åœ¨å¤„ç†ä¸­ï¼‰
        task_service.update_task_status(db, task_id, "processing")
        logger.info(f"ã€Part2 ä»»åŠ¡çŠ¶æ€å·²è®¾ç½®ä¸º processingã€‘taskId={task_id}")

        # 3. å‡†å¤‡ Part1 ä¸Šä¸‹æ–‡å’Œ style_summary
        part1_context = {
            "professional_evaluation_summary": task.part1_summary or "",
            "workflow_draft": json.loads(task.workflow_draft) if task.workflow_draft else {},
        }
        
        # ä» Part1 ç»“æœä¸­æå– style_summaryï¼ˆé£æ ¼å…‹éš†æˆ˜ç•¥æŒ‡å¯¼ï¼‰
        # è·¯å¾„ï¼šstructured_result.sections.photoReview.structured.photographerStyleSummary
        style_summary = ""
        if task.structured_result:
            try:
                sections = task.structured_result.get("sections", {})
                photo_review = sections.get("photoReview", {})
                structured = photo_review.get("structured", {})
                style_summary = structured.get("photographerStyleSummary", "")
                
                # å¦‚æœ photographerStyleSummary ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–è·¯å¾„æå–
                if not style_summary:
                    # å°è¯•ä» gemini_result ä¸­æå–ï¼ˆæ–° Prompt ç»“æ„ï¼šmodule_1_critique.style_summaryï¼‰
                    if task.gemini_result:
                        try:
                            module_1 = task.gemini_result.get("module_1_critique", {})
                            if isinstance(module_1, dict):
                                style_summary = module_1.get("style_summary", "")
                                if style_summary:
                                    logger.info(f"Part2 ä» gemini_result.module_1_critique.style_summary æå–åˆ° style_summary, taskId={task_id}")
                        except Exception as e:
                            logger.warning(f"Part2 ä» gemini_result æå– style_summary å¤±è´¥: {e}, taskId={task_id}")
                    
                    if not style_summary:
                        logger.warning(f"Part2 æœªæ‰¾åˆ° style_summaryï¼ŒPart2 å°†æ— æ³•ä½¿ç”¨ Phase 1 çš„æˆ˜ç•¥æŒ‡å¯¼, taskId={task_id}")
                
                logger.info(f"Part2 æå– style_summary é•¿åº¦: {len(style_summary) if style_summary else 0} å­—ç¬¦, taskId={task_id}")
                if style_summary:
                    logger.debug(f"Part2 style_summary å‰ 200 å­—ç¬¦: {style_summary[:200]}..., taskId={task_id}")
            except Exception as e:
                logger.error(f"Part2 æå– style_summary å¤±è´¥: {e}, taskId={task_id}", exc_info=True)
                style_summary = ""

        # 4. æ„å»º Prompt å’Œ Gemini API è¯·æ±‚å†…å®¹
        prompt = prompt_template.get_part2_prompt(
            task.source_image_data or "",
            task.target_image_data,
            part1_context,
            style_summary=style_summary,  # ä¼ é€’ style_summary
            feasibility_result=task.feasibility_result,
        )

        # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨æ¯å¼ å›¾ç‰‡å‰æ·»åŠ æ–‡æœ¬æ ‡è®°ï¼Œæ˜ç¡®æ ‡è¯†å›¾ç‰‡ç±»å‹ï¼Œé˜²æ­¢ Gemini æ··æ·†å›¾ç‰‡é¡ºåº
        contents = [{"role": "user", "parts": [{"text": prompt}]}]
        if task.source_image_data:
            # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨å‚è€ƒå›¾å‰æ·»åŠ æ–‡æœ¬æ ‡è®°
            contents[0]["parts"].append({
                "text": "ã€å›¾ç‰‡1ï¼šå‚è€ƒå›¾ï¼ˆReference Imageï¼‰ã€‘è¿™æ˜¯ç¬¬ä¸€å¼ å›¾ç‰‡ï¼Œæ˜¯ç›®æ ‡é£æ ¼å›¾ã€‚"
            })
            contents[0]["parts"].append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": task.source_image_data.split(",")[-1] if "," in task.source_image_data else task.source_image_data,
                }
            })
        if task.target_image_data:
            # ã€æ–¹æ¡ˆ2ï¼šå›¾ç‰‡æ ‡è®°ã€‘åœ¨ç”¨æˆ·å›¾å‰æ·»åŠ æ–‡æœ¬æ ‡è®°
            contents[0]["parts"].append({
                "text": "ã€å›¾ç‰‡2ï¼šç”¨æˆ·å›¾ï¼ˆUser Imageï¼‰ã€‘è¿™æ˜¯ç¬¬äºŒå¼ å›¾ç‰‡ï¼Œæ˜¯éœ€è¦å¤„ç†çš„å›¾ç‰‡ã€‚"
            })
            contents[0]["parts"].append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": task.target_image_data.split(",")[-1] if "," in task.target_image_data else task.target_image_data,
                }
            })
        
        # ã€æ–¹æ¡ˆ4ï¼šéªŒè¯æ—¥å¿—ã€‘è®°å½• Part2 å›¾ç‰‡é¡ºåº
        logger.info(f"ã€Part2 åˆ†æã€‘å›¾ç‰‡é¡ºåºç¡®è®¤ï¼šç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆå‚è€ƒå›¾ï¼‰base64é•¿åº¦={len(task.source_image_data.split(',')[-1]) if task.source_image_data and ',' in task.source_image_data else len(task.source_image_data) if task.source_image_data else 0}, ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆç”¨æˆ·å›¾ï¼‰base64é•¿åº¦={len(task.target_image_data.split(',')[-1]) if task.target_image_data and ',' in task.target_image_data else len(task.target_image_data) if task.target_image_data else 0}")

        # 5. è°ƒç”¨ Gemini API
        logger.info(f"Part2 å¼€å§‹è°ƒç”¨ Gemini API, taskId={task_id}")
        gemini_response = gemini_service.generate_text(contents, stage="part2")
        logger.info(f"Part2 Gemini API è°ƒç”¨å®Œæˆï¼Œå“åº”é•¿åº¦: {len(gemini_response)} å­—ç¬¦, taskId={task_id}")
        logger.debug(f"Part2 Gemini åŸå§‹å“åº”å‰ 500 å­—ç¬¦: {gemini_response[:500]}..., taskId={task_id}")
        
        # 6. ä¿å­˜ Gemini å“åº”åˆ°æ–‡ä»¶ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        timestamp = int(time.time())
        gemini_response_file = f"/tmp/gemini_response_part2_{timestamp}.json"
        try:
            with open(gemini_response_file, 'w', encoding='utf-8') as f:
                f.write(gemini_response)
            logger.info(f"Part2 Gemini å®Œæ•´å“åº”å·²ä¿å­˜åˆ°: {gemini_response_file}, taskId={task_id}")
        except Exception as save_error:
            logger.warning(f"Part2 ä¿å­˜ Gemini å“åº”åˆ°æ–‡ä»¶å¤±è´¥: {save_error}, taskId={task_id}")

        # 7. è§£æ Gemini JSON å“åº”
        try:
            gemini_json = json.loads(gemini_response)
            logger.info(f"Part2 Gemini JSON è§£ææˆåŠŸ: ç±»å‹ = {type(gemini_json)}, taskId={task_id}")
        except Exception as parse_error:
            logger.warning(f"Part2 Gemini JSON è§£æå¤±è´¥: {parse_error}, å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–, taskId={task_id}")
            import re
            json_match = re.search(r'\{.*\}', gemini_response, re.DOTALL)
            if json_match:
                try:
                    gemini_json = json.loads(json_match.group())
                    logger.info(f"Part2 Gemini JSON æ­£åˆ™æå–æˆåŠŸ: ç±»å‹ = {type(gemini_json)}, taskId={task_id}")
                except Exception as regex_error:
                    logger.error(f"Part2 Gemini JSON æ­£åˆ™æå–ä¹Ÿå¤±è´¥: {regex_error}, taskId={task_id}")
                    raise ValueError("æ— æ³•è§£æ Gemini è¿”å›çš„ JSON")
            else:
                logger.error(f"Part2 Gemini å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼çš„æ•°æ®, taskId={task_id}")
                raise ValueError("æ— æ³•è§£æ Gemini è¿”å›çš„ JSON")

        # 8. ä» Gemini å“åº”ä¸­æå– workflow_execution_summary å’Œ workflow_alignment_notes
        # ã€æ³¨æ„ã€‘æ–°çš„ Part2 Prompt ç»“æ„ä¸åŒ…å« workflow_execution_summary å­—æ®µ
        # æ–°æ ¼å¼åªåŒ…å« phase_1_extraction.style_summary_recap å’Œ phase_1_extraction.key_adjustment_strategy
        # ä¸ºäº†å‘åå…¼å®¹ï¼Œæˆ‘ä»¬ä»ç„¶å°è¯•æå–ï¼Œä½†å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²
        workflow_execution_summary = ""
        if isinstance(gemini_json, dict):
            # ä¼˜å…ˆä»æ–°æ ¼å¼ä¸­æå–ï¼ˆè™½ç„¶æ–°æ ¼å¼ä¸åŒ…å«æ­¤å­—æ®µï¼Œä½†ä¸ºäº†å‘åå…¼å®¹ä»å°è¯•ï¼‰
            phase_1_extraction = gemini_json.get("phase_1_extraction", {})
            if isinstance(phase_1_extraction, dict):
                workflow_execution_summary = phase_1_extraction.get("workflow_execution_summary", "")
            # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä»é¡¶å±‚è·å–ï¼ˆæ—§æ ¼å¼ï¼‰
            if not workflow_execution_summary:
                workflow_execution_summary = gemini_json.get("workflow_execution_summary", "")
        
        # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œå°è¯•ä» phase_1_extraction ä¸­ç»„åˆ style_summary_recap å’Œ key_adjustment_strategy
        # ä½œä¸º workflow_execution_summary çš„æ›¿ä»£ï¼ˆè™½ç„¶ä¸æ˜¯å®Œå…¨ç›¸åŒçš„å­—æ®µï¼Œä½†å¯ä»¥ä½œä¸ºå·¥ä½œæµæ‘˜è¦ï¼‰
        if not workflow_execution_summary and isinstance(gemini_json, dict):
            phase_1_extraction = gemini_json.get("phase_1_extraction", {})
            if isinstance(phase_1_extraction, dict):
                style_summary_recap = phase_1_extraction.get("style_summary_recap", "")
                key_adjustment_strategy = phase_1_extraction.get("key_adjustment_strategy", "")
                if style_summary_recap or key_adjustment_strategy:
                    workflow_execution_summary = f"{style_summary_recap}\n\n{key_adjustment_strategy}".strip()
                    logger.info(f"Part2 ä» phase_1_extraction ç»„åˆç”Ÿæˆ workflow_execution_summary, taskId={task_id}")
        
        # workflow_alignment_notes å¯èƒ½åœ¨æ–°æ ¼å¼ä¸­ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
        workflow_alignment_notes = gemini_json.get("workflow_alignment_notes", "") if isinstance(gemini_json, dict) else ""

        # 9. æ ¼å¼åŒ–æ•°æ®
        logger.info(f"Part2 å¼€å§‹æ ¼å¼åŒ–æ•°æ®: gemini_json ç±»å‹ = {type(gemini_json)}, keys = {list(gemini_json.keys()) if isinstance(gemini_json, dict) else 'not dict'}, taskId={task_id}")
        try:
            structured_result = formatter.format_part2(gemini_json, task.structured_result)
            logger.info(f"Part2 æ ¼å¼åŒ–æˆåŠŸ: structured_result keys = {list(structured_result.keys()) if isinstance(structured_result, dict) else 'not dict'}, taskId={task_id}")
            
            # ã€è¯¦ç»†æ—¥å¿—ã€‘è®°å½•æ ¼å¼åŒ–åçš„ sections ç»“æ„
            if isinstance(structured_result, dict) and "sections" in structured_result:
                sections = structured_result.get("sections", {})
                logger.info(f"Part2 sections keys: {list(sections.keys())}, taskId={task_id}")
                
                # æ£€æŸ¥ color section
                if "color" in sections:
                    color_section = sections.get("color", {})
                    color_structured = color_section.get("structured", {})
                    logger.info(f"Part2 color section: has structured = {bool(color_structured)}, structured keys = {list(color_structured.keys()) if isinstance(color_structured, dict) else 'not dict'}, taskId={task_id}")
                    logger.debug(f"Part2 color structured preview: whiteBalance = {bool(color_structured.get('whiteBalance'))}, grading = {bool(color_structured.get('grading'))}, hsl = {len(color_structured.get('hsl', []))} items, taskId={task_id}")
                    # ã€å…³é”®ã€‘æ£€æŸ¥ä¸‰ä¸ªæ–°å­—æ®µæ˜¯å¦å­˜åœ¨
                    logger.info(f"Part2 color phase_1_extraction å­—æ®µæ£€æŸ¥: master_style_recap = {bool(color_structured.get('master_style_recap'))}, style_summary_recap = {bool(color_structured.get('style_summary_recap'))}, key_adjustment_strategy = {bool(color_structured.get('key_adjustment_strategy'))}, taskId={task_id}")
                    if color_structured.get('master_style_recap'):
                        logger.info(f"Part2 color master_style_recap å†…å®¹é¢„è§ˆ: {color_structured.get('master_style_recap')[:100]}..., taskId={task_id}")
                    if color_structured.get('style_summary_recap'):
                        logger.info(f"Part2 color style_summary_recap å†…å®¹é¢„è§ˆ: {color_structured.get('style_summary_recap')[:100]}..., taskId={task_id}")
                    if color_structured.get('key_adjustment_strategy'):
                        logger.info(f"Part2 color key_adjustment_strategy å†…å®¹é¢„è§ˆ: {color_structured.get('key_adjustment_strategy')[:100]}..., taskId={task_id}")
                
                # æ£€æŸ¥ lightroom section
                if "lightroom" in sections:
                    lightroom_section = sections.get("lightroom", {})
                    lightroom_structured = lightroom_section.get("structured", {})
                    logger.info(f"Part2 lightroom section: has structured = {bool(lightroom_structured)}, structured keys = {list(lightroom_structured.keys()) if isinstance(lightroom_structured, dict) else 'not dict'}, taskId={task_id}")
                    logger.debug(f"Part2 lightroom structured preview: panels = {len(lightroom_structured.get('panels', []))} items, has toneCurve = {bool(lightroom_structured.get('toneCurve'))}, has colorGrading = {bool(lightroom_structured.get('colorGrading'))}, taskId={task_id}")
                
                # æ£€æŸ¥ photoshop section
                if "photoshop" in sections:
                    photoshop_section = sections.get("photoshop", {})
                    photoshop_structured = photoshop_section.get("structured", {})
                    logger.info(f"Part2 photoshop section: has structured = {bool(photoshop_structured)}, structured keys = {list(photoshop_structured.keys()) if isinstance(photoshop_structured, dict) else 'not dict'}, taskId={task_id}")
                    logger.debug(f"Part2 photoshop structured preview: steps = {len(photoshop_structured.get('steps', []))} items, taskId={task_id}")
        except Exception as format_error:
            logger.error(f"Part2 æ ¼å¼åŒ–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {format_error}, taskId={task_id}", exc_info=True)
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯ç»“æ„ï¼Œç¡®ä¿æ¥å£èƒ½æ­£å¸¸è¿”å›
            structured_result = {
                "protocolVersion": "2025-02",
                "stage": "part2",
                "meta": {
                    "warnings": [f"æ ¼å¼åŒ–å¤±è´¥: {str(format_error)}"],
                    "rawNaturalLanguage": "",
                },
                "sections": {
                    "lightroom": {
                        "naturalLanguage": {},
                        "structured": {
                            "panels": [],
                            "toneCurve": [[0, 0], [64, 64], [128, 128], [192, 192], [255, 255]],
                            "rgbCurves": {},
                            "colorGrading": {},
                            "localAdjustments": [],
                        },
                    },
                    "photoshop": {
                        "naturalLanguage": {},
                        "structured": {
                            "steps": [],
                        },
                    },
                    "color": {
                        "naturalLanguage": {},
                        "structured": {
                            "styleKey": "",
                            "whiteBalance": {
                                "temp": {"range": "+0"},
                                "tint": {"range": "+0"},
                            },
                            "grading": {
                                "highlights": {"hue": 0, "saturation": 0},
                                "midtones": {"hue": 0, "saturation": 0},
                                "shadows": {"hue": 0, "saturation": 0},
                                "balance": 0,
                            },
                            "hsl": [],
                        },
                    },
                },
            }

        # 10. å‡†å¤‡æ•°æ®åº“æ›´æ–°æ•°æ®
        logger.info(f"Part2 æ ¼å¼åŒ–å®Œæˆ: structured_result keys = {list(structured_result.keys()) if isinstance(structured_result, dict) else 'not dict'}, taskId={task_id}")
        logger.info(f"Part2 workflow_execution_summary é•¿åº¦: {len(workflow_execution_summary)} å­—ç¬¦, taskId={task_id}")
        logger.info(f"Part2 workflow_alignment_notes é•¿åº¦: {len(workflow_alignment_notes)} å­—ç¬¦, taskId={task_id}")

        # å°† workflow_execution_summary è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼ˆæ ¹æ®å¼€å‘æ–¹æ¡ˆï¼Œworkflow_final åº”å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
        workflow_final_json = json.dumps({"workflow_execution_summary": workflow_execution_summary}) if workflow_execution_summary else json.dumps({"workflow_execution_summary": ""})
        
        # 10. æ›´æ–°æ•°æ®åº“
        logger.info(f"Part2 å¼€å§‹æ›´æ–°æ•°æ®åº“..., taskId={task_id}")
        logger.info(f"Part2 æ›´æ–°æ•°æ®åº“å‚æ•°: taskId={task_id}, workflow_final_jsoné•¿åº¦={len(workflow_final_json)}, workflow_alignment_notesé•¿åº¦={len(workflow_alignment_notes)}")
        
        # ã€è¯¦ç»†æ—¥å¿—ã€‘è®°å½•è¦æ›´æ–°çš„ structured_result ç»“æ„
        if isinstance(structured_result, dict) and "sections" in structured_result:
            sections_to_update = structured_result.get("sections", {})
            logger.info(f"Part2 è¦æ›´æ–°çš„ sections keys: {list(sections_to_update.keys())}, taskId={task_id}")
            logger.debug(f"Part2 è¦æ›´æ–°çš„ sections è¯¦æƒ…: lightroom = {'å­˜åœ¨' if 'lightroom' in sections_to_update else 'ä¸å­˜åœ¨'}, photoshop = {'å­˜åœ¨' if 'photoshop' in sections_to_update else 'ä¸å­˜åœ¨'}, color = {'å­˜åœ¨' if 'color' in sections_to_update else 'ä¸å­˜åœ¨'}, taskId={task_id}")
        else:
            logger.warning(f"Part2 structured_result ä¸­æ²¡æœ‰ sections å­—æ®µ, taskId={task_id}")
        
        try:
            task_service.update_task_part2(
                db,
                task.id,
                gemini_json,
                structured_result,
                gemini_response,
                workflow_final_json,  # ä½¿ç”¨ JSON å­—ç¬¦ä¸²æ ¼å¼çš„ workflow_final
                workflow_alignment_notes,
            )
            logger.info(f"Part2 æ•°æ®åº“æ›´æ–°å®Œæˆ, taskId={task_id}")
        except Exception as db_error:
            logger.error(f"Part2 æ•°æ®åº“æ›´æ–°å¤±è´¥: {db_error}, taskId={task_id}", exc_info=True)
            # å³ä½¿æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œä¹Ÿå°è¯•å°†ä»»åŠ¡çŠ¶æ€è®¾ç½®ä¸ºå¤±è´¥ï¼Œå¹¶è®°å½•åŸå› 
            task_service.update_task_status(db, task_id, "failed", f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(db_error)}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Part2 æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(db_error)}")
        
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•åå°ä»»åŠ¡å®Œæˆæ—¶é—´å’Œæ€»è€—æ—¶
        job_elapsed_time = time.time() - job_start_time
        logger.info(f"ã€Part2 åå°ä»»åŠ¡æˆåŠŸã€‘taskId={task_id}, æ€»è€—æ—¶={job_elapsed_time:.2f}ç§’, ä»»åŠ¡çŠ¶æ€=completed")
    except Exception as e:
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•åå°ä»»åŠ¡å¤±è´¥æ—¶é—´å’Œæ€»è€—æ—¶
        job_elapsed_time = time.time() - job_start_time
        # ã€å¢å¼ºé”™è¯¯æ—¥å¿—ã€‘è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬é”™è¯¯ç±»å‹ã€é”™è¯¯æ¶ˆæ¯ã€å †æ ˆè·Ÿè¸ª
        error_type = type(e).__name__
        error_message = str(e)
        logger.error(f"ã€Part2 åå°ä»»åŠ¡å¤±è´¥ã€‘taskId={task_id}, é”™è¯¯ç±»å‹: {error_type}, é”™è¯¯æ¶ˆæ¯: {error_message}, è€—æ—¶={job_elapsed_time:.2f}ç§’", exc_info=True)
        
        # ã€å…³é”®ä¿®å¤ã€‘æ˜ç¡®æç¤ºä»£ç†è¿æ¥æ‹’ç»é”™è¯¯
        if "Connection refused" in error_message or "Errno 61" in error_message:
            error_message = "æ— æ³•è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ ClashX(7890) æˆ– Clash Verge(7897) æ˜¯å¦å·²å¯åŠ¨ï¼Œå¹¶ç¡®è®¤ç«¯å£é…ç½®æ­£ç¡®ã€‚"
            logger.error(f"ã€Part2 åå°ä»»åŠ¡å¤±è´¥ã€‘æ£€æµ‹åˆ°ä»£ç†è¿æ¥é”™è¯¯: {error_message}")
        
        # ã€æ„å»ºè¯¦ç»†çš„å¤±è´¥åŸå› ã€‘åŒ…å«é”™è¯¯ç±»å‹å’Œé”™è¯¯æ¶ˆæ¯ï¼Œä¾¿äºå‰ç«¯æ˜¾ç¤ºå’Œè°ƒè¯•
        status_reason = f"Part2 åå°åˆ†æå¤±è´¥: {error_type}: {error_message}"
        # å¦‚æœé”™è¯¯æ¶ˆæ¯è¿‡é•¿ï¼Œæˆªå–å‰ 500 ä¸ªå­—ç¬¦ï¼ˆé¿å…æ•°æ®åº“å­—æ®µè¿‡é•¿ï¼‰
        if len(status_reason) > 500:
            status_reason = status_reason[:500] + "..."
        
        # ç¡®ä¿ä»»åŠ¡çŠ¶æ€è¢«æ›´æ–°ä¸ºå¤±è´¥
        db_for_update: Session = next(get_db())
        try:
            task_service.update_task_status(db_for_update, task_id, "failed", status_reason)
            logger.info(f"ã€Part2 ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºå¤±è´¥ã€‘taskId={task_id}, status_reason={status_reason}")
        except Exception as status_error:
            logger.error(f"ã€Part2 æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥ã€‘taskId={task_id}, é”™è¯¯: {status_error}", exc_info=True)
        finally:
            db_for_update.close()
    finally:
        db.close()
        logger.info(f"ã€Part2 åå°ä»»åŠ¡ç»“æŸã€‘taskId={task_id}, æ•°æ®åº“ä¼šè¯å·²å…³é—­")


@router.get("/{taskId}")
async def get_task(
    taskId: str,
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """è·å–ä»»åŠ¡è¯¦æƒ…"""
    current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)
    
    task = task_service.get_task(db, taskId)
    if not task:
        raise error_response(ErrorCode.TASK_NOT_FOUND, "ä»»åŠ¡ä¸å­˜åœ¨")

    if task.user_id != current_user.id:
        raise error_response(ErrorCode.FORBIDDEN, "æ— æƒè®¿é—®æ­¤ä»»åŠ¡")

    # ã€è¿”å›ä»»åŠ¡è¯¦æƒ…ã€‘æ ¹æ®å¼€å‘æ–¹æ¡ˆï¼Œè¿”å›ä»»åŠ¡çŠ¶æ€å’Œç»“æœ
    # æ³¨æ„ï¼šstatus_reason å­—æ®µç”¨äºè®°å½•ä»»åŠ¡å¤±è´¥åŸå› ï¼ˆå¦‚æœä»»åŠ¡çŠ¶æ€ä¸º failedï¼‰
    # ã€è°ƒè¯•æ—¥å¿—ã€‘è®°å½•è¿”å›çš„ structured_result ç»“æ„ï¼Œç”¨äºæ’æŸ¥æ•°æ®ä¸¢å¤±é—®é¢˜
    if task.structured_result and isinstance(task.structured_result, dict):
        sections = task.structured_result.get("sections", {})
        if "lightroom" in sections:
            lightroom_section = sections.get("lightroom", {})
            lightroom_structured = lightroom_section.get("structured", {}) if isinstance(lightroom_section, dict) else {}
            lightroom_panels = lightroom_structured.get("panels", []) if isinstance(lightroom_structured, dict) else []
            logger.info(f"ã€get_taskã€‘è¿”å›çš„ lightroom section: has structured = {bool(lightroom_structured)}, panels count = {len(lightroom_panels) if isinstance(lightroom_panels, list) else 0}, taskId={taskId}")
            # ã€è¯¦ç»†æ£€æŸ¥ã€‘æ£€æŸ¥ panels çš„å†…å®¹æ˜¯å¦ä¸ºç©º
            if isinstance(lightroom_panels, list) and len(lightroom_panels) > 0:
                first_panel = lightroom_panels[0]
                has_content = bool(first_panel.get("title") or first_panel.get("description") or first_panel.get("params"))
                logger.debug(f"ã€get_taskã€‘lightroom ç¬¬ä¸€ä¸ª panel æ˜¯å¦æœ‰å†…å®¹: {has_content}, title = {first_panel.get('title')}, params count = {len(first_panel.get('params', []))}, taskId={taskId}")
                if not has_content:
                    logger.error(f"ã€get_taskã€‘âŒ lightroom panels å†…å®¹ä¸ºç©ºï¼ç¬¬ä¸€ä¸ª panel: {json.dumps(first_panel, ensure_ascii=False)[:200]}, taskId={taskId}")
    
    # ã€æ—¥å¿—è®°å½•ã€‘å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œè®°å½•å¤±è´¥åŸå› 
    if task.status == "failed":
        logger.warning(f"ã€get_taskã€‘âš ï¸ ä»»åŠ¡å¤±è´¥: taskId={taskId}, status_reason={task.status_reason if hasattr(task, 'status_reason') and task.status_reason else 'æœªæä¾›å¤±è´¥åŸå› '}")
    
    return {
        "code": 0,
        "message": "ok",
        "data": {
            "task": {
                "id": task.id,
                "status": task.status,
                "status_reason": task.status_reason if hasattr(task, 'status_reason') else None,  # ä»»åŠ¡å¤±è´¥åŸå› ï¼ˆå¯é€‰ï¼‰
                "feasibility_result": task.feasibility_result,
                "created_at": task.created_at.isoformat(),
            },
            "structuredResult": task.structured_result,
        },
    }


@router.get("/history")
async def get_history(
    limit: int = 20,
    page: int = 1,
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """è·å–å†å²ä»»åŠ¡åˆ—è¡¨"""
    current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)

    tasks = (
        db.query(AnalysisTask)
        .filter(AnalysisTask.user_id == current_user.id)
        .order_by(desc(AnalysisTask.created_at))
        .offset((page - 1) * limit)
        .limit(limit)
        .all()
    )

    # è®¡ç®—æ€»æ•°ï¼ˆç”¨äºåˆ†é¡µï¼‰
    total = db.query(func.count(AnalysisTask.id)).filter(
        AnalysisTask.user_id == current_user.id
    ).scalar() or 0

    return success_response(
        data={
            "items": [
                {
                    "taskId": t.id,
                    "created_at": t.created_at.isoformat(),
                    "status": t.status,
                    "feasibilityScore": t.feasibility_result.get("feasibilityScore") if t.feasibility_result else None,
                }
                for t in tasks
            ],
            "page": page,
            "pageSize": limit,
            "total": total,  # æ·»åŠ æ€»æ•°å­—æ®µï¼Œå‰ç«¯éœ€è¦ç”¨äºåˆ†é¡µ
        },
    )


@router.post("/diagnosis")
async def analyze_diagnosis(
    request_data: DiagnosisRequestSchema = Body(...),
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_db),
):
    """
    AI è¯Šæ–­æ¥å£
    æ ¹æ®è‰²å½©é›·è¾¾å’ŒAIè¯Šæ–­åŠŸèƒ½å®Œæ•´è®¾è®¡æ–¹æ¡ˆå®ç°
    æä¾›ä¸“ä¸šçš„æ‘„å½±è¯Šæ–­æŠ¥å‘Šï¼ˆå¤šç»´è¯„åˆ†ã€é—®é¢˜å®šä½ã€æ”¹è¿›å»ºè®®ï¼‰
    
    Args:
        request_data: è¯Šæ–­è¯·æ±‚æ•°æ®
            {
                "imageUrl": str,  # å›¾ç‰‡ URL æˆ– base64ï¼ˆä½åˆ†è¾¨ç‡ï¼Œå»ºè®® 512x512ï¼‰
                "histogramData": {
                    "r": [0, 1, 2, ...],  # 256 ä¸ªæ•´æ•°ï¼Œçº¢è‰²é€šé“åˆ†å¸ƒ
                    "g": [0, 1, 2, ...],  # ç»¿è‰²é€šé“åˆ†å¸ƒ
                    "b": [0, 1, 2, ...],  # è“è‰²é€šé“åˆ†å¸ƒ
                    "l": [0, 1, 2, ...],  # äº®åº¦åˆ†å¸ƒ
                    "avgL": 128,  # å¹³å‡äº®åº¦
                    "shadows": 0.2,  # æš—éƒ¨æ¯”ä¾‹
                    "midtones": 0.5,  # ä¸­é—´è°ƒæ¯”ä¾‹
                    "highlights": 0.8  # é«˜å…‰æ¯”ä¾‹
                },
                "dominantColors": [
                    {"h": 180, "s": 0.8, "v": 0.9, "hex": "#00FFFF"},
                    ...
                ],
                "taskId": str  # å¯é€‰ï¼Œå…³è”å·²æœ‰åˆ†æä»»åŠ¡
            }
        credentials: JWT Tokenï¼ˆBearerï¼Œå¿…å¡«ï¼‰
        db: æ•°æ®åº“ä¼šè¯
    
    Returns:
        {
            "code": 0,
            "message": "ok",
            "data": {
                "scores": {
                    "exposure": 8.5,  # 0-10 åˆ†
                    "color": 7.2,
                    "composition": 9.0,
                    "mood": 8.8
                },
                "critique": "é«˜å…‰éƒ¨åˆ†ç»†èŠ‚ä¸¢å¤±ä¸¥é‡ï¼Œå»ºè®®é™ä½æ›å…‰...",
                "suggestions": [
                    "å°è¯•å°†è‰²æ¸©æ»‘å—å‘å·¦ç§»åŠ¨ -500K",
                    "é™ä½é«˜å…‰å€¼ä»¥æ¢å¤å¤©ç©ºç»†èŠ‚"
                ],
                "issues": [
                    {
                        "type": "exposure",
                        "severity": "high",
                        "description": "é«˜å…‰æº¢å‡º",
                        "region": "sky"
                    }
                ],
                "processingTime": 2.5  # å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            }
        }
    
    Raises:
        HTTPException: å¦‚æœå‚æ•°éªŒè¯å¤±è´¥ã€ç”¨æˆ·æœªç™»å½•ã€æˆ–è¯Šæ–­è¿‡ç¨‹å‡ºé”™
    
    Note:
        - éœ€è¦ç™»å½•æ‰èƒ½ä½¿ç”¨
        - ä½¿ç”¨ Gemini å¤šæ¨¡æ€åˆ†æï¼ˆå›¾ç‰‡ + æ•°æ®ï¼‰
        - è¯Šæ–­ç»“æœå¯ä»¥ç¼“å­˜ï¼ˆç›¸åŒå›¾ç‰‡ + ç›¸åŒæ•°æ®ï¼‰
    """
    start_time = time.time()
    
    try:
        # ã€æ—¥å¿—è®°å½•ã€‘è®°å½•å‡½æ•°å…¥å£ï¼ˆä½¿ç”¨ INFO çº§åˆ«ï¼Œç¡®ä¿æ—¥å¿—è¢«è®°å½•ï¼‰
        logger.info("=" * 80)
        logger.info("ã€AI è¯Šæ–­ã€‘=========================================")
        logger.info(f"ã€AI è¯Šæ–­ã€‘å‡½æ•°è¢«è°ƒç”¨ï¼Œå¼€å§‹å¤„ç†è¯·æ±‚")
        logger.info(f"ã€AI è¯Šæ–­ã€‘è¯·æ±‚è·¯å¾„: /api/analyze/diagnosis")
        logger.info(f"ã€AI è¯Šæ–­ã€‘è¯·æ±‚æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ã€AI è¯Šæ–­ã€‘histogramData keys: {list(request_data.histogramData.keys())}")
        logger.info(f"ã€AI è¯Šæ–­ã€‘dominantColors æ•°é‡: {len(request_data.dominantColors)}")
        logger.info(f"ã€AI è¯Šæ–­ã€‘imageUrl é•¿åº¦: {len(request_data.imageUrl) if request_data.imageUrl else 0} å­—ç¬¦")
        logger.info("ã€AI è¯Šæ–­ã€‘=========================================")
        logger.info("=" * 80)
        
        # ã€èº«ä»½éªŒè¯ã€‘éªŒè¯ç”¨æˆ·èº«ä»½
        try:
            current_user = await get_current_user(credentials=credentials, db=db, require_admin=False)
            logger.info(f"ã€AI è¯Šæ–­ã€‘âœ… ç”¨æˆ·èº«ä»½éªŒè¯æˆåŠŸ: ç”¨æˆ· {current_user.email} (ID: {current_user.id})")
        except Exception as auth_error:
            logger.error(f"ã€AI è¯Šæ–­ã€‘âŒ ç”¨æˆ·èº«ä»½éªŒè¯å¤±è´¥: {type(auth_error).__name__}: {str(auth_error)}")
            raise
        
        # ã€å‚æ•°éªŒè¯ã€‘æ£€æŸ¥å¿…è¦å­—æ®µ
        if not request_data.imageUrl:
            logger.error("ã€AI è¯Šæ–­ã€‘å¤±è´¥: imageUrl å‚æ•°ä¸ºç©º")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "å›¾ç‰‡ URLï¼ˆimageUrlï¼‰ä¸èƒ½ä¸ºç©º")
        
        if not request_data.histogramData:
            logger.error("ã€AI è¯Šæ–­ã€‘å¤±è´¥: histogramData å‚æ•°ä¸ºç©º")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "ç›´æ–¹å›¾æ•°æ®ï¼ˆhistogramDataï¼‰ä¸èƒ½ä¸ºç©º")
        
        # ã€å‚æ•°éªŒè¯ã€‘æ£€æŸ¥ histogramData æ˜¯å¦ä¸ºç©ºå­—å…¸
        if isinstance(request_data.histogramData, dict) and len(request_data.histogramData) == 0:
            logger.warning("ã€AI è¯Šæ–­ã€‘histogramData ä¸ºç©ºå­—å…¸ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            # è®¾ç½®é»˜è®¤çš„ç›´æ–¹å›¾æ•°æ®ï¼Œé¿å…åç»­å¤„ç†å‡ºé”™
            request_data.histogramData = {
                "r": [0] * 256,
                "g": [0] * 256,
                "b": [0] * 256,
                "l": [0] * 256,
                "avgL": 128,
                "shadows": 0.2,
                "midtones": 0.5,
                "highlights": 0.8
            }
        
        # ã€æ„å»º Promptã€‘ä½¿ç”¨è¯Šæ–­ Prompt æ¨¡æ¿
        try:
            prompt = prompt_template.get_diagnosis_prompt(
                histogram_data=request_data.histogramData,
                dominant_colors=request_data.dominantColors
            )
            logger.debug(f"ã€AI è¯Šæ–­ã€‘Prompt ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
        except Exception as e:
            error_type = type(e).__name__
            error_detail = str(e)
            logger.error(f"ã€AI è¯Šæ–­ã€‘Prompt ç”Ÿæˆå¤±è´¥: {error_type}: {error_detail}", exc_info=True)
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Prompt ç”Ÿæˆå¤±è´¥: {error_detail}")
        
        # ã€æ„å»º Gemini è¯·æ±‚å†…å®¹ã€‘åŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡
        contents = [{"role": "user", "parts": [{"text": prompt}]}]
        
        # ã€å›¾ç‰‡å¤„ç†ã€‘æ·»åŠ å›¾ç‰‡ï¼ˆå¤„ç† base64 æˆ– data URLï¼‰
        image_data = request_data.imageUrl
        logger.debug(f"ã€AI è¯Šæ–­ã€‘åŸå§‹ imageUrl é•¿åº¦: {len(image_data)} å­—ç¬¦")
        logger.debug(f"ã€AI è¯Šæ–­ã€‘imageUrl å‰ç¼€: {image_data[:50] if len(image_data) > 50 else image_data}")
        
        if image_data.startswith("data:image"):
            # data URL æ ¼å¼ï¼šdata:image/jpeg;base64,...
            image_data = image_data.split(",")[-1]
            logger.debug(f"ã€AI è¯Šæ–­ã€‘æå– base64 æ•°æ®ï¼Œé•¿åº¦: {len(image_data)} å­—ç¬¦")
        else:
            logger.warning(f"ã€AI è¯Šæ–­ã€‘imageUrl ä¸æ˜¯ data URL æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
        
        # ã€éªŒè¯ã€‘ç¡®ä¿å›¾ç‰‡æ•°æ®ä¸ä¸ºç©º
        if not image_data or len(image_data) < 100:
            logger.error(f"ã€AI è¯Šæ–­ã€‘å›¾ç‰‡æ•°æ®æ— æ•ˆ: é•¿åº¦={len(image_data) if image_data else 0}")
            raise error_response(ErrorCode.MISSING_REQUIRED_FIELD, "å›¾ç‰‡æ•°æ®æ— æ•ˆæˆ–ä¸ºç©º")
        
        contents[0]["parts"].append({
            "inline_data": {
                "mime_type": "image/jpeg",
                "data": image_data,
            }
        })
        
        logger.debug(f"ã€AI è¯Šæ–­ã€‘Gemini è¯·æ±‚å†…å®¹æ„å»ºå®Œæˆï¼Œparts æ•°é‡: {len(contents[0]['parts'])}")
        
        # ã€è°ƒç”¨ Gemini APIã€‘è¿›è¡Œå¤šæ¨¡æ€åˆ†æ
        logger.info("ã€AI è¯Šæ–­ã€‘å¼€å§‹è°ƒç”¨ Gemini API...")
        
        # ã€éªŒè¯ã€‘æ£€æŸ¥ Gemini æœåŠ¡æ˜¯å¦å·²åˆå§‹åŒ–
        if not gemini_service or not gemini_service._client:
            logger.error("ã€AI è¯Šæ–­ã€‘Gemini æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ GEMINI_API_KEY é…ç½®")
            raise error_response(ErrorCode.INTERNAL_ERROR, "Gemini æœåŠ¡æœªé…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
        
        try:
            logger.info("ã€AI è¯Šæ–­ã€‘å¼€å§‹è°ƒç”¨ Gemini APIï¼Œé¢„è®¡è€—æ—¶ 30-60 ç§’...")
            gemini_start_time = time.time()
            gemini_response = gemini_service.generate_text(contents, stage="diagnosis", response_mime="application/json")
            gemini_duration = time.time() - gemini_start_time
            logger.info(f"ã€AI è¯Šæ–­ã€‘Gemini API è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(gemini_response)} å­—ç¬¦ï¼Œè€—æ—¶: {gemini_duration:.2f} ç§’")
        except RuntimeError as e:
            # Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–é”™è¯¯
            error_msg = str(e)
            logger.error(f"ã€AI è¯Šæ–­ã€‘Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–: {error_msg}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Gemini æœåŠ¡æœªé…ç½®: {error_msg}")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Gemini æœåŠ¡æœªé…ç½®: {error_msg}")
        except ConnectionError as conn_err:
            # ã€å…³é”®ä¿®å¤ã€‘æ˜ç¡®æç¤ºä»£ç†è¿æ¥æ‹’ç»é”™è¯¯
            error_detail = str(conn_err)
            if "Connection refused" in error_detail or "Errno 61" in error_detail:
                raise error_response(ErrorCode.INTERNAL_ERROR, "æ— æ³•è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ ClashX(7890) æˆ– Clash Verge(7897) æ˜¯å¦å·²å¯åŠ¨ï¼Œå¹¶ç¡®è®¤ç«¯å£é…ç½®æ­£ç¡®ã€‚")
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Gemini API è¿æ¥å¤±è´¥: {error_detail}")
        except Exception as e:
            # å…¶ä»– Gemini API è°ƒç”¨é”™è¯¯
            error_type = type(e).__name__
            error_detail = str(e)
            logger.error(f"ã€AI è¯Šæ–­ã€‘Gemini API è°ƒç”¨å¤±è´¥: {error_type}: {error_detail}", exc_info=True)
            raise error_response(ErrorCode.INTERNAL_ERROR, f"Gemini API è°ƒç”¨å¤±è´¥: {error_detail}")
        
        # ã€è§£æå’ŒéªŒè¯å“åº”ã€‘ä½¿ç”¨ Schema éªŒè¯
        logger.debug("ã€AI è¯Šæ–­ã€‘å¼€å§‹è§£æå’ŒéªŒè¯å“åº”...")
        logger.debug(f"ã€AI è¯Šæ–­ã€‘Gemini åŸå§‹å“åº”å‰ 500 å­—ç¬¦: {gemini_response[:500] if len(gemini_response) > 500 else gemini_response}")
        
        try:
            validated_result = validate_diagnosis_response(gemini_response)
            logger.info("ã€AI è¯Šæ–­ã€‘å“åº”éªŒè¯æˆåŠŸ")
        except Exception as e:
            # Schema éªŒè¯å¤±è´¥
            error_type = type(e).__name__
            error_detail = str(e)
            logger.error(f"ã€AI è¯Šæ–­ã€‘å“åº”éªŒè¯å¤±è´¥: {error_type}: {error_detail}", exc_info=True)
            logger.error(f"ã€AI è¯Šæ–­ã€‘Gemini åŸå§‹å“åº”: {gemini_response[:1000]}")
            # ã€ä¿®å¤ã€‘å³ä½¿éªŒè¯å¤±è´¥ï¼Œä¹Ÿè¿”å›é»˜è®¤ç»“æ„ï¼Œè€Œä¸æ˜¯æŠ›å‡º 500 é”™è¯¯
            # è¿™æ ·å‰ç«¯å¯ä»¥æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥
            # æ³¨æ„ï¼švalidate_diagnosis_response åœ¨éªŒè¯å¤±è´¥æ—¶ä¼šè¿”å›é»˜è®¤ç»“æ„ï¼Œä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            # ä½†å¦‚æœç¡®å®æŠ›å‡ºå¼‚å¸¸ï¼Œæˆ‘ä»¬éœ€è¦æ•è·å¹¶è¿”å›é»˜è®¤ç»“æ„
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤å€¼é‡æ–°éªŒè¯
                validated_result = validate_diagnosis_response(gemini_response)
            except:
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤ç»“æ„
                logger.warning(f"ã€AI è¯Šæ–­ã€‘ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤ç»“æ„è¿”å›ï¼ŒåŸå§‹å“åº”éªŒè¯å¤±è´¥")
                validated_result = {
                    "scores": {
                        "exposure": {"value": 5.0, "description": "æ— æ³•è§£æè¯„åˆ†"},
                        "color": {"value": 5.0, "description": "æ— æ³•è§£æè¯„åˆ†"},
                        "composition": {"value": 5.0, "description": "æ— æ³•è§£æè¯„åˆ†"},
                        "mood": {"value": 5.0, "description": "æ— æ³•è§£æè¯„åˆ†"}
                    },
                    "critique": "AI è¯Šæ–­å“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æç»“æœ",
                    "suggestions": ["è¯·é‡è¯•è¯Šæ–­"],
                    "issues": []
                }
        
        # ã€è®¡ç®—å¤„ç†æ—¶é—´ã€‘
        processing_time = time.time() - start_time
        validated_result["processingTime"] = round(processing_time, 2)
        
        logger.info(f"ã€AI è¯Šæ–­ã€‘âœ… è¯Šæ–­å®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        logger.info(f"ã€AI è¯Šæ–­ã€‘è¯„åˆ†: æ›å…‰={validated_result['scores']['exposure']}, è‰²å½©={validated_result['scores']['color']}, æ„å›¾={validated_result['scores']['composition']}, æƒ…æ„Ÿ={validated_result['scores']['mood']}")
        
        # ã€è¿”å›ç»“æœã€‘
        return success_response(
            data=validated_result,
            message="AI è¯Šæ–­å®Œæˆ"
        )
        
    except HTTPException:
        # é‡æ–°æŠ›å‡º HTTP å¼‚å¸¸ï¼ˆå¦‚è®¤è¯å¤±è´¥ã€å‚æ•°é”™è¯¯ç­‰ï¼‰
        raise
    except Exception as e:
        # ã€é”™è¯¯å¤„ç†ã€‘æ•è·æ‰€æœ‰æœªé¢„æœŸçš„å¼‚å¸¸
        error_type = type(e).__name__
        error_detail = str(e)
        error_traceback = None
        try:
            import traceback
            error_traceback = traceback.format_exc()
        except:
            pass
        
        logger.error(f"ã€AI è¯Šæ–­ã€‘âŒ è¯Šæ–­å¤±è´¥: {error_type}: {error_detail}")
        if error_traceback:
            logger.error(f"ã€AI è¯Šæ–­ã€‘é”™è¯¯å †æ ˆ:\n{error_traceback}")
        logger.error(f"ã€AI è¯Šæ–­ã€‘è¯·æ±‚æ•°æ®æ‘˜è¦: imageUrlé•¿åº¦={len(request_data.imageUrl) if request_data.imageUrl else 0}, histogramDataKeys={list(request_data.histogramData.keys()) if request_data.histogramData else []}, dominantColorsCount={len(request_data.dominantColors)}")
        
        # è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•
        raise error_response(ErrorCode.INTERNAL_ERROR, f"AI è¯Šæ–­å¤±è´¥: {error_type}: {error_detail}")
